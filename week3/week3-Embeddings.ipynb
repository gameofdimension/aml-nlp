{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "File data/test.tsv is already downloaded.\n",
      "File data/test_embeddings.tsv is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. You need to download it by following this [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load_word2vec_format in module gensim.models.keyedvectors:\n",
      "\n",
      "load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) method of builtins.type instance\n",
      "    Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      "    \n",
      "    Note that the information stored in the file is incomplete (the binary tree is missing),\n",
      "    so while you can query for word similarity etc., you cannot continue training\n",
      "    with a model loaded this way.\n",
      "    \n",
      "    `binary` is a boolean indicating whether the data is in binary word2vec format.\n",
      "    `norm_only` is a boolean indicating whether to only store normalised word2vec vectors in memory.\n",
      "    Word counts are read from `fvocab` filename, if set (this is the file generated\n",
      "    by `-save-vocab` flag of the original C tool).\n",
      "    \n",
      "    If you trained the C model using non-utf8 encoding for words, specify that\n",
      "    encoding in `encoding`.\n",
      "    \n",
      "    `unicode_errors`, default 'strict', is a string suitable to be passed as the `errors`\n",
      "    argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      "    file may include word tokens truncated in the middle of a multibyte unicode character\n",
      "    (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      "    \n",
      "    `limit` sets a maximum number of word-vectors to read from the file. The default,\n",
      "    None, means read all.\n",
      "    \n",
      "    `datatype` (experimental) can coerce dimensions to a non-default float type (such\n",
      "    as np.float16) to save memory. (Such types may result in much slower bulk operations\n",
      "    or incompatibility with optimized routines.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "help(models.KeyedVectors.load_word2vec_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin',\n",
    "                                                         binary=True, limit=500000)\n",
    "######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    \n",
    "    vec = np.zeros((dim,), dtype=np.float32)\n",
    "    count = 0\n",
    "    for w in question.split():\n",
    "        if w in embeddings:\n",
    "            count += 1\n",
    "            vec += embeddings[w]\n",
    "    if count == 0:\n",
    "        return vec\n",
    "    return vec/count\n",
    "            \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891266\n",
      "-0.0287272129208\n",
      "0.0460561104119\n",
      "0.0852593332529\n",
      "0.0243055559695\n",
      "-0.0729031041265\n",
      "0.0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list with ranks for each duplicate (the best rank is 1, the worst is len(dup_ranks))\n",
    "        k: number of top-ranked elements\n",
    "\n",
    "        result: float number\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    count = 0\n",
    "    for rank in dup_ranks:\n",
    "        if rank <=k:\n",
    "            count += 1\n",
    "    return count/(len(dup_ranks)+1e-8)\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list with ranks for each duplicate (the best rank is 1, the worst is len(dup_ranks))\n",
    "        k: number of top-ranked elements\n",
    "\n",
    "        result: float number\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    score = 0\n",
    "    for rank in dup_ranks:\n",
    "        if rank <= k:\n",
    "            score += 1/np.log2(1+rank)\n",
    "    return score/(len(dup_ranks)+1e-8)\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 0.9999999900000002\n",
      "0.4999999975\n",
      "0.999999995\n",
      "0.4999999975\n",
      "0.999999995\n",
      "0.3333333322222222\n",
      "0.6666666644...\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 0.99999999\n",
      "0.4999999975\n",
      "0.815464872708\n",
      "0.4999999975\n",
      "0.815464872708\n",
      "0.333333332222\n",
      "0.543643249378\n",
      "0.7...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')\n",
    "######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    # print(\"ffffffff\")\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    vecq = np.array([np.array(question_to_vec(question, embeddings, dim))])\n",
    "    vecc = np.array([np.array(question_to_vec(can, embeddings, dim)) for can in candidates])\n",
    "    scores = list(cosine_similarity(vecq, vecc)[0])\n",
    "    tl = [(i, candidates[i], scores[i]) for i in range(len(candidates))]\n",
    "    stl = sorted(tl, key=lambda x:x[2], reverse=True)\n",
    "    return [(t[0],t[1]) for t in stl]\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.212 | Hits@   1: 0.212\n",
      "DCG@   5: 0.267 | Hits@   5: 0.315\n",
      "DCG@  10: 0.282 | Hits@  10: 0.363\n",
      "DCG@ 100: 0.320 | Hits@ 100: 0.552\n",
      "DCG@ 500: 0.353 | Hits@ 500: 0.811\n",
      "DCG@1000: 0.373 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    q = text_prepare(q)\n",
    "    for i,e in enumerate(ex):\n",
    "        ex[i] = text_prepare(e)\n",
    "    prepared_validation.append([q,*ex])\n",
    "    ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.310 | Hits@   1: 0.310\n",
      "DCG@   5: 0.380 | Hits@   5: 0.443\n",
      "DCG@  10: 0.397 | Hits@  10: 0.494\n",
      "DCG@ 100: 0.430 | Hits@ 100: 0.661\n",
      "DCG@ 500: 0.453 | Hits@ 500: 0.835\n",
      "DCG@1000: 0.470 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "prepare_file('./data/validation.tsv', './data/tp_v.tsv')\n",
    "prepare_file('./data/test.tsv', './data/tp_t.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file('./data/train.tsv', './data/tp_train.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = './data/tp_t.tsv' ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace could not be run on Windows and we recommend to use provided\n",
    "docker container or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a nuber of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : ./data/tp_train.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : ./data/tp_train.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73.4%  lr: 0.042993  loss: 0.009974  eta: 0h8m  tot: 0h1m28s  (14.7%)0%  lr: 0.049940  loss: 0.047360  eta: 0h13m  tot: 0h0m1s  (0.2%)%  lr: 0.049920  loss: 0.044684  eta: 0h13m  tot: 0h0m2s  (0.3%)2.1%  lr: 0.049870  loss: 0.037615  eta: 0h11m  tot: 0h0m2s  (0.4%)3.5%  lr: 0.049720  loss: 0.031830  eta: 0h10m  tot: 0h0m4s  (0.7%)3.6%  lr: 0.049710  loss: 0.031552  eta: 0h10m  tot: 0h0m4s  (0.7%)4.3%  lr: 0.049600  loss: 0.029458  eta: 0h10m  tot: 0h0m5s  (0.9%)4.4%  lr: 0.049600  loss: 0.029193  eta: 0h10m  tot: 0h0m5s  (0.9%)4.9%  lr: 0.049580  loss: 0.028103  eta: 0h10m  tot: 0h0m6s  (1.0%)5.0%  lr: 0.049550  loss: 0.027648  eta: 0h10m  tot: 0h0m6s  (1.0%)6.0%  lr: 0.049449  loss: 0.025782  eta: 0h10m  tot: 0h0m7s  (1.2%)7.7%  lr: 0.049259  loss: 0.023623  eta: 0h10m  tot: 0h0m9s  (1.5%)8.3%  lr: 0.049229  loss: 0.022892  eta: 0h10m  tot: 0h0m10s  (1.7%)8.4%  lr: 0.049219  loss: 0.022744  eta: 0h10m  tot: 0h0m10s  (1.7%)%  lr: 0.049019  loss: 0.021411  eta: 0h10m  tot: 0h0m12s  (2.0%)10.6%  lr: 0.048949  loss: 0.020721  eta: 0h10m  tot: 0h0m13s  (2.1%)10.7%  lr: 0.048949  loss: 0.020640  eta: 0h10m  tot: 0h0m13s  (2.1%)10.9%  lr: 0.048919  loss: 0.020538  eta: 0h10m  tot: 0h0m13s  (2.2%)11.0%  lr: 0.048899  loss: 0.020513  eta: 0h10m  tot: 0h0m13s  (2.2%)11.3%  lr: 0.048899  loss: 0.020309  eta: 0h10m  tot: 0h0m14s  (2.3%)11.3%  lr: 0.048899  loss: 0.020285  eta: 0h10m  tot: 0h0m14s  (2.3%)11.4%  lr: 0.048889  loss: 0.020226  eta: 0h10m  tot: 0h0m14s  (2.3%)11.5%  lr: 0.048889  loss: 0.020214  eta: 0h10m  tot: 0h0m14s  (2.3%)11.6%  lr: 0.048879  loss: 0.020133  eta: 0h10m  tot: 0h0m14s  (2.3%)12.2%  lr: 0.048839  loss: 0.019841  eta: 0h10m  tot: 0h0m15s  (2.4%)12.3%  lr: 0.048819  loss: 0.019734  eta: 0h10m  tot: 0h0m15s  (2.5%)13.8%  lr: 0.048689  loss: 0.018890  eta: 0h10m  tot: 0h0m17s  (2.8%)14.4%  lr: 0.048609  loss: 0.018579  eta: 0h10m  tot: 0h0m18s  (2.9%)14.8%  lr: 0.048599  loss: 0.018384  eta: 0h10m  tot: 0h0m18s  (3.0%)16.2%  lr: 0.048398  loss: 0.017654  eta: 0h10m  tot: 0h0m20s  (3.2%)16.6%  lr: 0.048328  loss: 0.017562  eta: 0h10m  tot: 0h0m20s  (3.3%)18.5%  lr: 0.048198  loss: 0.016818  eta: 0h10m  tot: 0h0m23s  (3.7%)19.4%  lr: 0.048088  loss: 0.016578  eta: 0h10m  tot: 0h0m24s  (3.9%)19.5%  lr: 0.048058  loss: 0.016563  eta: 0h9m  tot: 0h0m24s  (3.9%)19.8%  lr: 0.048048  loss: 0.016494  eta: 0h9m  tot: 0h0m24s  (4.0%)20.3%  lr: 0.048038  loss: 0.016319  eta: 0h9m  tot: 0h0m25s  (4.1%)20.6%  lr: 0.048008  loss: 0.016233  eta: 0h9m  tot: 0h0m25s  (4.1%)21.2%  lr: 0.047918  loss: 0.016039  eta: 0h9m  tot: 0h0m26s  (4.2%)21.5%  lr: 0.047908  loss: 0.015935  eta: 0h9m  tot: 0h0m26s  (4.3%)22.6%  lr: 0.047788  loss: 0.015646  eta: 0h10m  tot: 0h0m28s  (4.5%)23.2%  lr: 0.047728  loss: 0.015529  eta: 0h10m  tot: 0h0m29s  (4.6%)23.3%  lr: 0.047718  loss: 0.015488  eta: 0h10m  tot: 0h0m29s  (4.7%)23.6%  lr: 0.047698  loss: 0.015448  eta: 0h10m  tot: 0h0m29s  (4.7%)23.8%  lr: 0.047678  loss: 0.015378  eta: 0h10m  tot: 0h0m30s  (4.8%)23.9%  lr: 0.047648  loss: 0.015339  eta: 0h10m  tot: 0h0m30s  (4.8%)24.0%  lr: 0.047648  loss: 0.015322  eta: 0h10m  tot: 0h0m30s  (4.8%)%  lr: 0.047578  loss: 0.015167  eta: 0h9m  tot: 0h0m31s  (4.9%)%  lr: 0.047578  loss: 0.015139  eta: 0h9m  tot: 0h0m31s  (5.0%)25.0%  lr: 0.047558  loss: 0.015074  eta: 0h9m  tot: 0h0m31s  (5.0%)%  lr: 0.047437  loss: 0.014755  eta: 0h9m  tot: 0h0m33s  (5.3%)27.1%  lr: 0.047397  loss: 0.014606  eta: 0h9m  tot: 0h0m34s  (5.4%)27.2%  lr: 0.047387  loss: 0.014583  eta: 0h9m  tot: 0h0m34s  (5.4%)27.5%  lr: 0.047377  loss: 0.014531  eta: 0h9m  tot: 0h0m34s  (5.5%)27.9%  lr: 0.047347  loss: 0.014465  eta: 0h9m  tot: 0h0m35s  (5.6%)28.5%  lr: 0.047247  loss: 0.014361  eta: 0h9m  tot: 0h0m36s  (5.7%)31.4%  lr: 0.047037  loss: 0.013777  eta: 0h9m  tot: 0h0m39s  (6.3%)32.2%  lr: 0.047017  loss: 0.013656  eta: 0h9m  tot: 0h0m40s  (6.4%)32.9%  lr: 0.046987  loss: 0.013515  eta: 0h9m  tot: 0h0m40s  (6.6%)34.8%  lr: 0.046827  loss: 0.013290  eta: 0h9m  tot: 0h0m43s  (7.0%)34.9%  lr: 0.046807  loss: 0.013279  eta: 0h9m  tot: 0h0m43s  (7.0%)35.1%  lr: 0.046787  loss: 0.013261  eta: 0h9m  tot: 0h0m43s  (7.0%)35.2%  lr: 0.046787  loss: 0.013243  eta: 0h9m  tot: 0h0m43s  (7.0%)37.2%  lr: 0.046577  loss: 0.012959  eta: 0h9m  tot: 0h0m45s  (7.4%)37.4%  lr: 0.046577  loss: 0.012937  eta: 0h9m  tot: 0h0m45s  (7.5%)37.7%  lr: 0.046537  loss: 0.012882  eta: 0h9m  tot: 0h0m46s  (7.5%)37.9%  lr: 0.046507  loss: 0.012851  eta: 0h9m  tot: 0h0m46s  (7.6%)41.0%  lr: 0.046186  loss: 0.012505  eta: 0h9m  tot: 0h0m50s  (8.2%)41.8%  lr: 0.046126  loss: 0.012400  eta: 0h9m  tot: 0h0m51s  (8.4%)42.6%  lr: 0.046076  loss: 0.012314  eta: 0h9m  tot: 0h0m51s  (8.5%)43.5%  lr: 0.046016  loss: 0.012221  eta: 0h9m  tot: 0h0m52s  (8.7%)43.6%  lr: 0.046016  loss: 0.012209  eta: 0h9m  tot: 0h0m53s  (8.7%)44.1%  lr: 0.045976  loss: 0.012163  eta: 0h9m  tot: 0h0m53s  (8.8%)44.2%  lr: 0.045966  loss: 0.012150  eta: 0h9m  tot: 0h0m53s  (8.8%)44.4%  lr: 0.045956  loss: 0.012123  eta: 0h9m  tot: 0h0m54s  (8.9%)45.7%  lr: 0.045746  loss: 0.011961  eta: 0h9m  tot: 0h0m55s  (9.1%)47.2%  lr: 0.045596  loss: 0.011796  eta: 0h9m  tot: 0h0m57s  (9.4%)47.5%  lr: 0.045556  loss: 0.011754  eta: 0h9m  tot: 0h0m57s  (9.5%)48.2%  lr: 0.045506  loss: 0.011704  eta: 0h9m  tot: 0h0m58s  (9.6%)%  lr: 0.045496  loss: 0.011693  eta: 0h9m  tot: 0h0m58s  (9.7%)48.7%  lr: 0.045466  loss: 0.011663  eta: 0h9m  tot: 0h0m59s  (9.7%)48.8%  lr: 0.045456  loss: 0.011656  eta: 0h9m  tot: 0h0m59s  (9.8%)49.5%  lr: 0.045365  loss: 0.011572  eta: 0h9m  tot: 0h1m0s  (9.9%)49.6%  lr: 0.045345  loss: 0.011574  eta: 0h9m  tot: 0h1m0s  (9.9%)51.9%  lr: 0.045115  loss: 0.011382  eta: 0h9m  tot: 0h1m2s  (10.4%)52.6%  lr: 0.045035  loss: 0.011326  eta: 0h9m  tot: 0h1m3s  (10.5%)53.4%  lr: 0.044985  loss: 0.011276  eta: 0h9m  tot: 0h1m4s  (10.7%)54.8%  lr: 0.044905  loss: 0.011141  eta: 0h9m  tot: 0h1m6s  (11.0%)55.4%  lr: 0.044845  loss: 0.011113  eta: 0h9m  tot: 0h1m7s  (11.1%)56.5%  lr: 0.044685  loss: 0.011058  eta: 0h8m  tot: 0h1m8s  (11.3%)%  lr: 0.044645  loss: 0.011039  eta: 0h8m  tot: 0h1m9s  (11.4%)m  tot: 0h1m10s  (11.5%)57.7%  lr: 0.044605  loss: 0.010996  eta: 0h8m  tot: 0h1m10s  (11.5%)58.1%  lr: 0.044565  loss: 0.010960  eta: 0h8m  tot: 0h1m10s  (11.6%)58.5%  lr: 0.044525  loss: 0.010929  eta: 0h8m  tot: 0h1m11s  (11.7%)59.0%  lr: 0.044495  loss: 0.010886  eta: 0h8m  tot: 0h1m11s  (11.8%)59.1%  lr: 0.044475  loss: 0.010874  eta: 0h8m  tot: 0h1m12s  (11.8%)%  lr: 0.044475  loss: 0.010865  eta: 0h8m  tot: 0h1m12s  (11.8%)59.7%  lr: 0.044435  loss: 0.010829  eta: 0h8m  tot: 0h1m12s  (11.9%)60.1%  lr: 0.044394  loss: 0.010796  eta: 0h8m  tot: 0h1m13s  (12.0%)60.7%  lr: 0.044324  loss: 0.010758  eta: 0h8m  tot: 0h1m13s  (12.1%)60.9%  lr: 0.044284  loss: 0.010745  eta: 0h8m  tot: 0h1m14s  (12.2%)61.5%  lr: 0.044144  loss: 0.010697  eta: 0h8m  tot: 0h1m14s  (12.3%)61.6%  lr: 0.044124  loss: 0.010699  eta: 0h8m  tot: 0h1m15s  (12.3%)62.4%  lr: 0.044064  loss: 0.010650  eta: 0h8m  tot: 0h1m15s  (12.5%)62.9%  lr: 0.044024  loss: 0.010599  eta: 0h8m  tot: 0h1m16s  (12.6%)63.3%  lr: 0.044004  loss: 0.010570  eta: 0h8m  tot: 0h1m16s  (12.7%)63.6%  lr: 0.043954  loss: 0.010548  eta: 0h8m  tot: 0h1m17s  (12.7%)65.7%  lr: 0.043714  loss: 0.010407  eta: 0h8m  tot: 0h1m20s  (13.1%)66.5%  lr: 0.043674  loss: 0.010352  eta: 0h8m  tot: 0h1m21s  (13.3%)67.1%  lr: 0.043584  loss: 0.010314  eta: 0h8m  tot: 0h1m21s  (13.4%)67.2%  lr: 0.043584  loss: 0.010312  eta: 0h8m  tot: 0h1m21s  (13.4%)%  lr: 0.043574  loss: 0.010300  eta: 0h8m  tot: 0h1m22s  (13.5%)70.1%  lr: 0.043273  loss: 0.010132  eta: 0h8m  tot: 0h1m25s  (14.0%)70.4%  lr: 0.043243  loss: 0.010121  eta: 0h8m  tot: 0h1m25s  (14.1%)70.5%  lr: 0.043223  loss: 0.010118  eta: 0h8m  tot: 0h1m25s  (14.1%)70.8%  lr: 0.043223  loss: 0.010099  eta: 0h8m  tot: 0h1m26s  (14.2%)72.4%  lr: 0.043093  loss: 0.010031  eta: 0h8m  tot: 0h1m27s  (14.5%)72.5%  lr: 0.043083  loss: 0.010026  eta: 0h8m  tot: 0h1m27s  (14.5%)73.2%  lr: 0.043013  loss: 0.009987  eta: 0h8m  tot: 0h1m28s  (14.6%)73.3%  lr: 0.043003  loss: 0.009980  eta: 0h8m  tot: 0h1m28s  (14.7%)73.5%  lr: 0.042973  loss: 0.009968  eta: 0h8m  tot: 0h1m28s  (14.7%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.040250  loss: 0.008897  eta: 0h8m  tot: 0h2m0s  (20.0%)74.1%  lr: 0.042933  loss: 0.009931  eta: 0h8m  tot: 0h1m29s  (14.8%)  eta: 0h8m  tot: 0h1m30s  (14.9%)75.1%  lr: 0.042833  loss: 0.009894  eta: 0h8m  tot: 0h1m31s  (15.0%)m31s  (15.1%)%  lr: 0.042653  loss: 0.009821  eta: 0h8m  tot: 0h1m33s  (15.4%)77.4%  lr: 0.042613  loss: 0.009797  eta: 0h8m  tot: 0h1m34s  (15.5%)77.8%  lr: 0.042603  loss: 0.009784  eta: 0h8m  tot: 0h1m34s  (15.6%)77.9%  lr: 0.042603  loss: 0.009783  eta: 0h8m  tot: 0h1m34s  (15.6%)78.2%  lr: 0.042543  loss: 0.009772  eta: 0h8m  tot: 0h1m35s  (15.6%)78.3%  lr: 0.042533  loss: 0.009772  eta: 0h8m  tot: 0h1m35s  (15.7%)79.6%  lr: 0.042463  loss: 0.009716  eta: 0h8m  tot: 0h1m36s  (15.9%)80.0%  lr: 0.042433  loss: 0.009691  eta: 0h8m  tot: 0h1m37s  (16.0%)80.5%  lr: 0.042383  loss: 0.009674  eta: 0h8m  tot: 0h1m37s  (16.1%)80.7%  lr: 0.042362  loss: 0.009664  eta: 0h8m  tot: 0h1m38s  (16.1%)80.9%  lr: 0.042342  loss: 0.009654  eta: 0h8m  tot: 0h1m38s  (16.2%)81.1%  lr: 0.042332  loss: 0.009642  eta: 0h8m  tot: 0h1m38s  (16.2%)81.7%  lr: 0.042302  loss: 0.009612  eta: 0h8m  tot: 0h1m39s  (16.3%)82.1%  lr: 0.042242  loss: 0.009594  eta: 0h8m  tot: 0h1m39s  (16.4%)82.6%  lr: 0.042132  loss: 0.009578  eta: 0h8m  tot: 0h1m40s  (16.5%)83.2%  lr: 0.042032  loss: 0.009548  eta: 0h8m  tot: 0h1m40s  (16.6%)83.5%  lr: 0.042022  loss: 0.009538  eta: 0h8m  tot: 0h1m41s  (16.7%)85.0%  lr: 0.041922  loss: 0.009471  eta: 0h8m  tot: 0h1m42s  (17.0%)85.2%  lr: 0.041902  loss: 0.009461  eta: 0h8m  tot: 0h1m43s  (17.0%)85.3%  lr: 0.041902  loss: 0.009456  eta: 0h8m  tot: 0h1m43s  (17.1%)85.9%  lr: 0.041822  loss: 0.009434  eta: 0h8m  tot: 0h1m43s  (17.2%)86.1%  lr: 0.041792  loss: 0.009432  eta: 0h8m  tot: 0h1m44s  (17.2%)86.2%  lr: 0.041772  loss: 0.009422  eta: 0h8m  tot: 0h1m44s  (17.2%)86.8%  lr: 0.041672  loss: 0.009405  eta: 0h8m  tot: 0h1m45s  (17.4%)87.1%  lr: 0.041632  loss: 0.009385  eta: 0h8m  tot: 0h1m45s  (17.4%)87.2%  lr: 0.041632  loss: 0.009381  eta: 0h8m  tot: 0h1m45s  (17.4%)87.6%  lr: 0.041572  loss: 0.009370  eta: 0h8m  tot: 0h1m46s  (17.5%)87.9%  lr: 0.041552  loss: 0.009356  eta: 0h8m  tot: 0h1m46s  (17.6%)89.1%  lr: 0.041422  loss: 0.009301  eta: 0h8m  tot: 0h1m47s  (17.8%)89.3%  lr: 0.041422  loss: 0.009293  eta: 0h8m  tot: 0h1m47s  (17.9%)89.4%  lr: 0.041422  loss: 0.009289  eta: 0h8m  tot: 0h1m47s  (17.9%)89.9%  lr: 0.041361  loss: 0.009255  eta: 0h8m  tot: 0h1m48s  (18.0%)90.0%  lr: 0.041311  loss: 0.009252  eta: 0h8m  tot: 0h1m48s  (18.0%)90.6%  lr: 0.041271  loss: 0.009237  eta: 0h8m  tot: 0h1m49s  (18.1%)90.7%  lr: 0.041241  loss: 0.009231  eta: 0h8m  tot: 0h1m49s  (18.1%)90.9%  lr: 0.041241  loss: 0.009225  eta: 0h8m  tot: 0h1m49s  (18.2%)91.8%  lr: 0.041131  loss: 0.009193  eta: 0h8m  tot: 0h1m50s  (18.4%)92.8%  lr: 0.041041  loss: 0.009159  eta: 0h8m  tot: 0h1m51s  (18.6%)93.1%  lr: 0.041011  loss: 0.009142  eta: 0h8m  tot: 0h1m51s  (18.6%)93.6%  lr: 0.040911  loss: 0.009124  eta: 0h8m  tot: 0h1m52s  (18.7%)94.6%  lr: 0.040791  loss: 0.009096  eta: 0h8m  tot: 0h1m53s  (18.9%)94.9%  lr: 0.040761  loss: 0.009081  eta: 0h8m  tot: 0h1m53s  (19.0%)95.2%  lr: 0.040721  loss: 0.009060  eta: 0h8m  tot: 0h1m54s  (19.0%)95.8%  lr: 0.040661  loss: 0.009041  eta: 0h8m  tot: 0h1m55s  (19.2%)96.0%  lr: 0.040651  loss: 0.009032  eta: 0h8m  tot: 0h1m55s  (19.2%)96.5%  lr: 0.040601  loss: 0.009018  eta: 0h8m  tot: 0h1m56s  (19.3%)96.8%  lr: 0.040591  loss: 0.009007  eta: 0h8m  tot: 0h1m56s  (19.4%)96.9%  lr: 0.040581  loss: 0.009002  eta: 0h8m  tot: 0h1m56s  (19.4%)99.1%  lr: 0.040330  loss: 0.008922  eta: 0h8m  tot: 0h1m59s  (19.8%)99.5%  lr: 0.040320  loss: 0.008912  eta: 0h8m  tot: 0h1m59s  (19.9%)\n",
      " ---+++                Epoch    0 Train error : 0.00900093 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57.5%  lr: 0.034515  loss: 0.002708  eta: 0h6m  tot: 0h3m10s  (31.5%)7%  lr: 0.039920  loss: 0.003003  eta: 0h6m  tot: 0h2m5s  (20.1%)0.9%  lr: 0.039880  loss: 0.002698  eta: 0h6m  tot: 0h2m5s  (20.2%)1.0%  lr: 0.039880  loss: 0.002523  eta: 0h7m  tot: 0h2m5s  (20.2%)1.1%  lr: 0.039860  loss: 0.002405  eta: 0h7m  tot: 0h2m5s  (20.2%)2.3%  lr: 0.039760  loss: 0.002445  eta: 0h7m  tot: 0h2m7s  (20.5%)%  lr: 0.039720  loss: 0.002411  eta: 0h7m  tot: 0h2m7s  (20.5%)2.9%  lr: 0.039710  loss: 0.002455  eta: 0h7m  tot: 0h2m7s  (20.6%)3.8%  lr: 0.039590  loss: 0.002354  eta: 0h7m  tot: 0h2m8s  (20.8%)4.4%  lr: 0.039500  loss: 0.002315  eta: 0h7m  tot: 0h2m9s  (20.9%)4.7%  lr: 0.039489  loss: 0.002289  eta: 0h7m  tot: 0h2m9s  (20.9%)5.0%  lr: 0.039449  loss: 0.002306  eta: 0h7m  tot: 0h2m10s  (21.0%)5.7%  lr: 0.039439  loss: 0.002304  eta: 0h7m  tot: 0h2m10s  (21.1%)6.0%  lr: 0.039379  loss: 0.002311  eta: 0h7m  tot: 0h2m11s  (21.2%)6.3%  lr: 0.039339  loss: 0.002345  eta: 0h7m  tot: 0h2m11s  (21.3%)7.6%  lr: 0.039269  loss: 0.002444  eta: 0h7m  tot: 0h2m12s  (21.5%)7.7%  lr: 0.039269  loss: 0.002455  eta: 0h7m  tot: 0h2m12s  (21.5%)%  lr: 0.039249  loss: 0.002451  eta: 0h7m  tot: 0h2m13s  (21.6%)8.2%  lr: 0.039239  loss: 0.002433  eta: 0h7m  tot: 0h2m13s  (21.6%)9.1%  lr: 0.039169  loss: 0.002390  eta: 0h7m  tot: 0h2m14s  (21.8%)9.2%  lr: 0.039169  loss: 0.002386  eta: 0h7m  tot: 0h2m14s  (21.8%)9.5%  lr: 0.039139  loss: 0.002422  eta: 0h7m  tot: 0h2m15s  (21.9%)11.2%  lr: 0.039029  loss: 0.002459  eta: 0h7m  tot: 0h2m16s  (22.2%)11.5%  lr: 0.038969  loss: 0.002461  eta: 0h7m  tot: 0h2m17s  (22.3%)11.8%  lr: 0.038969  loss: 0.002470  eta: 0h7m  tot: 0h2m17s  (22.4%)12.1%  lr: 0.038959  loss: 0.002471  eta: 0h7m  tot: 0h2m17s  (22.4%)12.2%  lr: 0.038929  loss: 0.002467  eta: 0h7m  tot: 0h2m18s  (22.4%)12.3%  lr: 0.038929  loss: 0.002480  eta: 0h7m  tot: 0h2m18s  (22.5%)12.5%  lr: 0.038919  loss: 0.002469  eta: 0h7m  tot: 0h2m18s  (22.5%)12.6%  lr: 0.038909  loss: 0.002461  eta: 0h7m  tot: 0h2m18s  (22.5%)13.5%  lr: 0.038819  loss: 0.002445  eta: 0h7m  tot: 0h2m19s  (22.7%)13.6%  lr: 0.038809  loss: 0.002450  eta: 0h7m  tot: 0h2m19s  (22.7%)14.3%  lr: 0.038729  loss: 0.002476  eta: 0h7m  tot: 0h2m20s  (22.9%)14.6%  lr: 0.038709  loss: 0.002459  eta: 0h7m  tot: 0h2m20s  (22.9%)%  lr: 0.038649  loss: 0.002483  eta: 0h7m  tot: 0h2m21s  (23.0%)15.4%  lr: 0.038599  loss: 0.002486  eta: 0h7m  tot: 0h2m21s  (23.1%)15.6%  lr: 0.038569  loss: 0.002509  eta: 0h7m  tot: 0h2m22s  (23.1%)16.6%  lr: 0.038479  loss: 0.002471  eta: 0h7m  tot: 0h2m23s  (23.3%)17.4%  lr: 0.038388  loss: 0.002474  eta: 0h7m  tot: 0h2m24s  (23.5%)17.6%  lr: 0.038338  loss: 0.002469  eta: 0h7m  tot: 0h2m24s  (23.5%)19.4%  lr: 0.038218  loss: 0.002470  eta: 0h7m  tot: 0h2m26s  (23.9%)19.5%  lr: 0.038208  loss: 0.002467  eta: 0h7m  tot: 0h2m26s  (23.9%)19.8%  lr: 0.038208  loss: 0.002464  eta: 0h7m  tot: 0h2m26s  (24.0%)20.4%  lr: 0.038178  loss: 0.002463  eta: 0h7m  tot: 0h2m27s  (24.1%)20.5%  lr: 0.038168  loss: 0.002469  eta: 0h7m  tot: 0h2m27s  (24.1%)21.1%  lr: 0.038118  loss: 0.002497  eta: 0h7m  tot: 0h2m28s  (24.2%)21.2%  lr: 0.038118  loss: 0.002506  eta: 0h7m  tot: 0h2m28s  (24.2%)21.9%  lr: 0.038048  loss: 0.002512  eta: 0h7m  tot: 0h2m29s  (24.4%)23.1%  lr: 0.037948  loss: 0.002504  eta: 0h7m  tot: 0h2m30s  (24.6%)24.3%  lr: 0.037878  loss: 0.002524  eta: 0h7m  tot: 0h2m31s  (24.9%)24.4%  lr: 0.037868  loss: 0.002529  eta: 0h7m  tot: 0h2m32s  (24.9%)24.7%  lr: 0.037838  loss: 0.002527  eta: 0h7m  tot: 0h2m32s  (24.9%)%  lr: 0.037788  loss: 0.002542  eta: 0h7m  tot: 0h2m32s  (25.0%)25.7%  lr: 0.037728  loss: 0.002551  eta: 0h7m  tot: 0h2m33s  (25.1%)26.2%  lr: 0.037678  loss: 0.002547  eta: 0h7m  tot: 0h2m34s  (25.2%)26.6%  lr: 0.037628  loss: 0.002540  eta: 0h7m  tot: 0h2m34s  (25.3%)26.7%  lr: 0.037608  loss: 0.002542  eta: 0h7m  tot: 0h2m34s  (25.3%)26.8%  lr: 0.037578  loss: 0.002542  eta: 0h7m  tot: 0h2m34s  (25.4%)27.3%  lr: 0.037538  loss: 0.002560  eta: 0h7m  tot: 0h2m35s  (25.5%)27.6%  lr: 0.037478  loss: 0.002560  eta: 0h7m  tot: 0h2m35s  (25.5%)27.7%  lr: 0.037457  loss: 0.002561  eta: 0h7m  tot: 0h2m36s  (25.5%)27.9%  lr: 0.037437  loss: 0.002565  eta: 0h7m  tot: 0h2m36s  (25.6%)28.2%  lr: 0.037377  loss: 0.002559  eta: 0h7m  tot: 0h2m36s  (25.6%)28.4%  lr: 0.037377  loss: 0.002577  eta: 0h7m  tot: 0h2m36s  (25.7%)28.9%  lr: 0.037297  loss: 0.002574  eta: 0h7m  tot: 0h2m37s  (25.8%)29.1%  lr: 0.037277  loss: 0.002568  eta: 0h7m  tot: 0h2m37s  (25.8%)29.3%  lr: 0.037237  loss: 0.002575  eta: 0h7m  tot: 0h2m38s  (25.9%)29.8%  lr: 0.037187  loss: 0.002569  eta: 0h7m  tot: 0h2m38s  (26.0%)30.0%  lr: 0.037137  loss: 0.002573  eta: 0h7m  tot: 0h2m38s  (26.0%)30.2%  lr: 0.037107  loss: 0.002580  eta: 0h7m  tot: 0h2m38s  (26.0%)31.1%  lr: 0.037017  loss: 0.002583  eta: 0h7m  tot: 0h2m40s  (26.2%)31.9%  lr: 0.036967  loss: 0.002590  eta: 0h7m  tot: 0h2m41s  (26.4%)32.0%  lr: 0.036967  loss: 0.002595  eta: 0h7m  tot: 0h2m41s  (26.4%)32.1%  lr: 0.036947  loss: 0.002604  eta: 0h7m  tot: 0h2m41s  (26.4%)32.5%  lr: 0.036917  loss: 0.002606  eta: 0h7m  tot: 0h2m41s  (26.5%)33.2%  lr: 0.036917  loss: 0.002617  eta: 0h7m  tot: 0h2m42s  (26.6%)33.6%  lr: 0.036877  loss: 0.002617  eta: 0h7m  tot: 0h2m43s  (26.7%)34.3%  lr: 0.036817  loss: 0.002616  eta: 0h6m  tot: 0h2m43s  (26.9%)35.5%  lr: 0.036707  loss: 0.002619  eta: 0h6m  tot: 0h2m44s  (27.1%)35.7%  lr: 0.036687  loss: 0.002624  eta: 0h6m  tot: 0h2m45s  (27.1%)35.9%  lr: 0.036657  loss: 0.002630  eta: 0h6m  tot: 0h2m45s  (27.2%)37.6%  lr: 0.036517  loss: 0.002629  eta: 0h6m  tot: 0h2m47s  (27.5%)37.8%  lr: 0.036507  loss: 0.002631  eta: 0h6m  tot: 0h2m47s  (27.6%)38.0%  lr: 0.036507  loss: 0.002637  eta: 0h6m  tot: 0h2m48s  (27.6%)38.2%  lr: 0.036497  loss: 0.002637  eta: 0h6m  tot: 0h2m48s  (27.6%)38.4%  lr: 0.036467  loss: 0.002643  eta: 0h6m  tot: 0h2m48s  (27.7%)39.3%  lr: 0.036376  loss: 0.002650  eta: 0h6m  tot: 0h2m49s  (27.9%)39.7%  lr: 0.036366  loss: 0.002643  eta: 0h6m  tot: 0h2m50s  (27.9%)39.9%  lr: 0.036336  loss: 0.002638  eta: 0h6m  tot: 0h2m50s  (28.0%)40.8%  lr: 0.036256  loss: 0.002621  eta: 0h6m  tot: 0h2m51s  (28.2%)40.9%  lr: 0.036236  loss: 0.002629  eta: 0h6m  tot: 0h2m51s  (28.2%)41.5%  lr: 0.036146  loss: 0.002633  eta: 0h6m  tot: 0h2m52s  (28.3%)41.6%  lr: 0.036146  loss: 0.002633  eta: 0h6m  tot: 0h2m52s  (28.3%)42.3%  lr: 0.036056  loss: 0.002649  eta: 0h6m  tot: 0h2m53s  (28.5%)43.1%  lr: 0.035946  loss: 0.002657  eta: 0h6m  tot: 0h2m54s  (28.6%)43.4%  lr: 0.035926  loss: 0.002660  eta: 0h6m  tot: 0h2m54s  (28.7%)43.8%  lr: 0.035896  loss: 0.002661  eta: 0h6m  tot: 0h2m54s  (28.8%)43.9%  lr: 0.035876  loss: 0.002663  eta: 0h6m  tot: 0h2m54s  (28.8%)44.5%  lr: 0.035816  loss: 0.002667  eta: 0h6m  tot: 0h2m55s  (28.9%)%  lr: 0.035766  loss: 0.002663  eta: 0h6m  tot: 0h2m56s  (29.0%)45.0%  lr: 0.035756  loss: 0.002670  eta: 0h6m  tot: 0h2m56s  (29.0%)45.6%  lr: 0.035706  loss: 0.002671  eta: 0h6m  tot: 0h2m56s  (29.1%)46.7%  lr: 0.035626  loss: 0.002686  eta: 0h6m  tot: 0h2m58s  (29.3%)%  lr: 0.035436  loss: 0.002694  eta: 0h6m  tot: 0h2m59s  (29.6%)48.1%  lr: 0.035425  loss: 0.002696  eta: 0h6m  tot: 0h2m59s  (29.6%)%  lr: 0.035425  loss: 0.002697  eta: 0h6m  tot: 0h2m59s  (29.7%)48.3%  lr: 0.035425  loss: 0.002698  eta: 0h6m  tot: 0h2m59s  (29.7%)48.9%  lr: 0.035365  loss: 0.002699  eta: 0h6m  tot: 0h3m0s  (29.8%)50.3%  lr: 0.035165  loss: 0.002690  eta: 0h6m  tot: 0h3m2s  (30.1%)50.5%  lr: 0.035135  loss: 0.002687  eta: 0h6m  tot: 0h3m2s  (30.1%)50.8%  lr: 0.035115  loss: 0.002691  eta: 0h6m  tot: 0h3m2s  (30.2%)52.5%  lr: 0.034975  loss: 0.002694  eta: 0h6m  tot: 0h3m4s  (30.5%)52.6%  lr: 0.034955  loss: 0.002694  eta: 0h6m  tot: 0h3m5s  (30.5%)54.8%  lr: 0.034725  loss: 0.002686  eta: 0h6m  tot: 0h3m7s  (31.0%)55.4%  lr: 0.034655  loss: 0.002687  eta: 0h6m  tot: 0h3m8s  (31.1%)55.9%  lr: 0.034635  loss: 0.002692  eta: 0h6m  tot: 0h3m9s  (31.2%)56.2%  lr: 0.034615  loss: 0.002692  eta: 0h6m  tot: 0h3m9s  (31.2%)56.8%  lr: 0.034545  loss: 0.002704  eta: 0h6m  tot: 0h3m9s  (31.4%)%  lr: 0.034525  loss: 0.002708  eta: 0h6m  tot: 0h3m10s  (31.5%)57.6%  lr: 0.034505  loss: 0.002712  eta: 0h6m  tot: 0h3m10s  (31.5%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030431  loss: 0.002693  eta: 0h5m  tot: 0h4m0s  (40.0%)%  lr: 0.034455  loss: 0.002709  eta: 0h6m  tot: 0h3m11s  (31.6%)58.9%  lr: 0.034354  loss: 0.002706  eta: 0h6m  tot: 0h3m12s  (31.8%)31.8%)59.3%  lr: 0.034304  loss: 0.002705  eta: 0h6m  tot: 0h3m13s  (31.9%)59.8%  lr: 0.034274  loss: 0.002703  eta: 0h6m  tot: 0h3m13s  (32.0%)61.0%  lr: 0.034164  loss: 0.002701  eta: 0h6m  tot: 0h3m14s  (32.2%)61.8%  lr: 0.034074  loss: 0.002690  eta: 0h6m  tot: 0h3m16s  (32.4%)62.3%  lr: 0.034024  loss: 0.002693  eta: 0h6m  tot: 0h3m16s  (32.5%)62.7%  lr: 0.033984  loss: 0.002695  eta: 0h6m  tot: 0h3m17s  (32.5%)62.9%  lr: 0.033954  loss: 0.002693  eta: 0h6m  tot: 0h3m17s  (32.6%)63.2%  lr: 0.033934  loss: 0.002698  eta: 0h6m  tot: 0h3m17s  (32.6%)%  lr: 0.033804  loss: 0.002704  eta: 0h6m  tot: 0h3m18s  (32.8%)64.1%  lr: 0.033754  loss: 0.002699  eta: 0h6m  tot: 0h3m18s  (32.8%)64.4%  lr: 0.033724  loss: 0.002693  eta: 0h6m  tot: 0h3m18s  (32.9%)64.5%  lr: 0.033724  loss: 0.002692  eta: 0h6m  tot: 0h3m19s  (32.9%)66.2%  lr: 0.033564  loss: 0.002691  eta: 0h6m  tot: 0h3m21s  (33.2%)66.6%  lr: 0.033524  loss: 0.002689  eta: 0h6m  tot: 0h3m21s  (33.3%)68.3%  lr: 0.033404  loss: 0.002686  eta: 0h6m  tot: 0h3m23s  (33.7%)69.6%  lr: 0.033283  loss: 0.002688  eta: 0h6m  tot: 0h3m25s  (33.9%)69.9%  lr: 0.033263  loss: 0.002691  eta: 0h6m  tot: 0h3m25s  (34.0%)70.1%  lr: 0.033253  loss: 0.002691  eta: 0h6m  tot: 0h3m26s  (34.0%)71.2%  lr: 0.033173  loss: 0.002695  eta: 0h6m  tot: 0h3m27s  (34.2%)72.0%  lr: 0.033123  loss: 0.002697  eta: 0h6m  tot: 0h3m28s  (34.4%)72.7%  lr: 0.033063  loss: 0.002696  eta: 0h6m  tot: 0h3m29s  (34.5%)73.5%  lr: 0.033013  loss: 0.002702  eta: 0h6m  tot: 0h3m29s  (34.7%)73.7%  lr: 0.033003  loss: 0.002703  eta: 0h6m  tot: 0h3m30s  (34.7%)74.7%  lr: 0.032943  loss: 0.002717  eta: 0h6m  tot: 0h3m31s  (34.9%)74.9%  lr: 0.032893  loss: 0.002720  eta: 0h6m  tot: 0h3m31s  (35.0%)75.5%  lr: 0.032783  loss: 0.002713  eta: 0h6m  tot: 0h3m32s  (35.1%)76.2%  lr: 0.032743  loss: 0.002713  eta: 0h6m  tot: 0h3m32s  (35.2%)76.4%  lr: 0.032703  loss: 0.002711  eta: 0h6m  tot: 0h3m33s  (35.3%)76.5%  lr: 0.032673  loss: 0.002710  eta: 0h6m  tot: 0h3m33s  (35.3%)76.7%  lr: 0.032673  loss: 0.002709  eta: 0h6m  tot: 0h3m33s  (35.3%)77.3%  lr: 0.032613  loss: 0.002717  eta: 0h6m  tot: 0h3m34s  (35.5%)78.1%  lr: 0.032513  loss: 0.002728  eta: 0h6m  tot: 0h3m35s  (35.6%)78.4%  lr: 0.032483  loss: 0.002724  eta: 0h6m  tot: 0h3m35s  (35.7%)78.5%  lr: 0.032483  loss: 0.002725  eta: 0h6m  tot: 0h3m35s  (35.7%)79.0%  lr: 0.032423  loss: 0.002730  eta: 0h6m  tot: 0h3m36s  (35.8%)79.1%  lr: 0.032403  loss: 0.002731  eta: 0h6m  tot: 0h3m36s  (35.8%)79.9%  lr: 0.032282  loss: 0.002725  eta: 0h6m  tot: 0h3m37s  (36.0%)80.8%  lr: 0.032202  loss: 0.002722  eta: 0h6m  tot: 0h3m38s  (36.2%)80.8%  lr: 0.032202  loss: 0.002721  eta: 0h6m  tot: 0h3m38s  (36.2%)81.3%  lr: 0.032172  loss: 0.002718  eta: 0h6m  tot: 0h3m38s  (36.3%)81.5%  lr: 0.032162  loss: 0.002722  eta: 0h6m  tot: 0h3m39s  (36.3%)82.9%  lr: 0.032062  loss: 0.002715  eta: 0h6m  tot: 0h3m40s  (36.6%)83.0%  lr: 0.032052  loss: 0.002714  eta: 0h6m  tot: 0h3m41s  (36.6%)83.5%  lr: 0.031982  loss: 0.002718  eta: 0h6m  tot: 0h3m41s  (36.7%)%  lr: 0.031962  loss: 0.002718  eta: 0h6m  tot: 0h3m42s  (36.8%)84.5%  lr: 0.031892  loss: 0.002716  eta: 0h6m  tot: 0h3m42s  (36.9%)84.8%  lr: 0.031872  loss: 0.002717  eta: 0h6m  tot: 0h3m42s  (37.0%)85.7%  lr: 0.031812  loss: 0.002710  eta: 0h6m  tot: 0h3m43s  (37.1%)85.9%  lr: 0.031792  loss: 0.002709  eta: 0h6m  tot: 0h3m44s  (37.2%)86.6%  lr: 0.031772  loss: 0.002707  eta: 0h6m  tot: 0h3m44s  (37.3%)87.4%  lr: 0.031652  loss: 0.002703  eta: 0h6m  tot: 0h3m45s  (37.5%)87.6%  lr: 0.031652  loss: 0.002699  eta: 0h6m  tot: 0h3m45s  (37.5%)87.7%  lr: 0.031632  loss: 0.002698  eta: 0h6m  tot: 0h3m46s  (37.5%)88.3%  lr: 0.031592  loss: 0.002700  eta: 0h6m  tot: 0h3m46s  (37.7%)90.6%  lr: 0.031432  loss: 0.002704  eta: 0h5m  tot: 0h3m49s  (38.1%)92.1%  lr: 0.031251  loss: 0.002696  eta: 0h5m  tot: 0h3m51s  (38.4%)92.5%  lr: 0.031201  loss: 0.002697  eta: 0h5m  tot: 0h3m51s  (38.5%)92.5%  lr: 0.031201  loss: 0.002698  eta: 0h5m  tot: 0h3m51s  (38.5%)92.8%  lr: 0.031181  loss: 0.002701  eta: 0h5m  tot: 0h3m51s  (38.6%)93.3%  lr: 0.031131  loss: 0.002703  eta: 0h5m  tot: 0h3m52s  (38.7%)94.1%  lr: 0.031061  loss: 0.002700  eta: 0h5m  tot: 0h3m53s  (38.8%)94.6%  lr: 0.031001  loss: 0.002705  eta: 0h5m  tot: 0h3m54s  (38.9%)95.8%  lr: 0.030891  loss: 0.002699  eta: 0h5m  tot: 0h3m55s  (39.2%)96.1%  lr: 0.030851  loss: 0.002700  eta: 0h5m  tot: 0h3m55s  (39.2%)96.3%  lr: 0.030811  loss: 0.002697  eta: 0h5m  tot: 0h3m55s  (39.3%)96.4%  lr: 0.030801  loss: 0.002698  eta: 0h5m  tot: 0h3m55s  (39.3%)98.1%  lr: 0.030671  loss: 0.002692  eta: 0h5m  tot: 0h3m57s  (39.6%)98.8%  lr: 0.030581  loss: 0.002696  eta: 0h5m  tot: 0h3m58s  (39.8%)99.3%  lr: 0.030521  loss: 0.002695  eta: 0h5m  tot: 0h3m59s  (39.9%)\n",
      " ---+++                Epoch    1 Train error : 0.00267491 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68.3%  lr: 0.023454  loss: 0.001948  eta: 0h4m  tot: 0h5m23s  (53.7%)4%  lr: 0.029990  loss: 0.002258  eta: 0h2m  tot: 0h4m5s  (40.1%)0.6%  lr: 0.029970  loss: 0.002311  eta: 0h3m  tot: 0h4m5s  (40.1%)2.6%  lr: 0.029840  loss: 0.002218  eta: 0h4m  tot: 0h4m7s  (40.5%)3.5%  lr: 0.029730  loss: 0.002071  eta: 0h4m  tot: 0h4m8s  (40.7%)3.6%  lr: 0.029730  loss: 0.002038  eta: 0h4m  tot: 0h4m8s  (40.7%)4.0%  lr: 0.029700  loss: 0.002088  eta: 0h4m  tot: 0h4m9s  (40.8%)4.4%  lr: 0.029660  loss: 0.002060  eta: 0h5m  tot: 0h4m9s  (40.9%)5.2%  lr: 0.029570  loss: 0.001982  eta: 0h5m  tot: 0h4m10s  (41.0%)5.4%  lr: 0.029560  loss: 0.001970  eta: 0h5m  tot: 0h4m10s  (41.1%)5.6%  lr: 0.029530  loss: 0.001962  eta: 0h5m  tot: 0h4m11s  (41.1%)5.9%  lr: 0.029510  loss: 0.001943  eta: 0h5m  tot: 0h4m11s  (41.2%)5.9%  lr: 0.029489  loss: 0.001961  eta: 0h5m  tot: 0h4m11s  (41.2%)6.4%  lr: 0.029459  loss: 0.001912  eta: 0h5m  tot: 0h4m12s  (41.3%)7.6%  lr: 0.029319  loss: 0.001884  eta: 0h5m  tot: 0h4m13s  (41.5%)9.6%  lr: 0.029089  loss: 0.001954  eta: 0h5m  tot: 0h4m16s  (41.9%)10.6%  lr: 0.029009  loss: 0.001946  eta: 0h5m  tot: 0h4m17s  (42.1%)11.3%  lr: 0.028909  loss: 0.001924  eta: 0h5m  tot: 0h4m17s  (42.3%)%  lr: 0.028879  loss: 0.001927  eta: 0h5m  tot: 0h4m18s  (42.3%)11.6%  lr: 0.028859  loss: 0.001923  eta: 0h5m  tot: 0h4m18s  (42.3%)11.9%  lr: 0.028809  loss: 0.001929  eta: 0h5m  tot: 0h4m18s  (42.4%)12.5%  lr: 0.028769  loss: 0.001955  eta: 0h5m  tot: 0h4m19s  (42.5%)s  (42.6%)13.7%  lr: 0.028659  loss: 0.001931  eta: 0h5m  tot: 0h4m20s  (42.7%)13.9%  lr: 0.028639  loss: 0.001946  eta: 0h5m  tot: 0h4m20s  (42.8%)14.0%  lr: 0.028639  loss: 0.001969  eta: 0h5m  tot: 0h4m20s  (42.8%)14.9%  lr: 0.028589  loss: 0.001994  eta: 0h5m  tot: 0h4m21s  (43.0%)15.6%  lr: 0.028438  loss: 0.001986  eta: 0h5m  tot: 0h4m22s  (43.1%)15.8%  lr: 0.028438  loss: 0.001989  eta: 0h5m  tot: 0h4m23s  (43.2%)16.0%  lr: 0.028428  loss: 0.001983  eta: 0h5m  tot: 0h4m23s  (43.2%)17.0%  lr: 0.028328  loss: 0.001982  eta: 0h5m  tot: 0h4m24s  (43.4%)17.3%  lr: 0.028318  loss: 0.001981  eta: 0h5m  tot: 0h4m24s  (43.5%)18.8%  lr: 0.028168  loss: 0.001970  eta: 0h5m  tot: 0h4m26s  (43.8%)19.2%  lr: 0.028118  loss: 0.001974  eta: 0h5m  tot: 0h4m26s  (43.8%)21.0%  lr: 0.027978  loss: 0.001964  eta: 0h5m  tot: 0h4m28s  (44.2%)21.8%  lr: 0.027918  loss: 0.001960  eta: 0h5m  tot: 0h4m29s  (44.4%)21.9%  lr: 0.027918  loss: 0.001957  eta: 0h5m  tot: 0h4m29s  (44.4%)23.0%  lr: 0.027798  loss: 0.001972  eta: 0h5m  tot: 0h4m31s  (44.6%)23.0%  lr: 0.027798  loss: 0.001976  eta: 0h5m  tot: 0h4m31s  (44.6%)23.2%  lr: 0.027788  loss: 0.001972  eta: 0h5m  tot: 0h4m31s  (44.6%)23.8%  lr: 0.027698  loss: 0.001954  eta: 0h5m  tot: 0h4m32s  (44.8%)24.3%  lr: 0.027678  loss: 0.001941  eta: 0h5m  tot: 0h4m32s  (44.9%)25.7%  lr: 0.027598  loss: 0.001941  eta: 0h5m  tot: 0h4m34s  (45.1%)25.8%  lr: 0.027578  loss: 0.001933  eta: 0h5m  tot: 0h4m34s  (45.2%)26.6%  lr: 0.027447  loss: 0.001922  eta: 0h5m  tot: 0h4m35s  (45.3%)26.8%  lr: 0.027397  loss: 0.001926  eta: 0h5m  tot: 0h4m35s  (45.4%)26.9%  lr: 0.027397  loss: 0.001925  eta: 0h5m  tot: 0h4m35s  (45.4%)27.8%  lr: 0.027307  loss: 0.001917  eta: 0h5m  tot: 0h4m36s  (45.6%)28.5%  lr: 0.027237  loss: 0.001916  eta: 0h5m  tot: 0h4m37s  (45.7%)30.5%  lr: 0.027077  loss: 0.001919  eta: 0h5m  tot: 0h4m40s  (46.1%)31.0%  lr: 0.027047  loss: 0.001915  eta: 0h5m  tot: 0h4m40s  (46.2%)31.1%  lr: 0.027017  loss: 0.001914  eta: 0h5m  tot: 0h4m41s  (46.2%)31.5%  lr: 0.026977  loss: 0.001920  eta: 0h5m  tot: 0h4m41s  (46.3%)32.2%  lr: 0.026927  loss: 0.001919  eta: 0h5m  tot: 0h4m42s  (46.4%)32.5%  lr: 0.026907  loss: 0.001915  eta: 0h5m  tot: 0h4m42s  (46.5%)33.5%  lr: 0.026837  loss: 0.001917  eta: 0h5m  tot: 0h4m43s  (46.7%)34.4%  lr: 0.026727  loss: 0.001925  eta: 0h5m  tot: 0h4m44s  (46.9%)34.5%  lr: 0.026717  loss: 0.001924  eta: 0h5m  tot: 0h4m44s  (46.9%)35.6%  lr: 0.026617  loss: 0.001916  eta: 0h5m  tot: 0h4m46s  (47.1%)36.0%  lr: 0.026587  loss: 0.001919  eta: 0h5m  tot: 0h4m46s  (47.2%)36.3%  lr: 0.026517  loss: 0.001913  eta: 0h5m  tot: 0h4m47s  (47.3%)36.5%  lr: 0.026487  loss: 0.001915  eta: 0h5m  tot: 0h4m47s  (47.3%)37.2%  lr: 0.026447  loss: 0.001920  eta: 0h5m  tot: 0h4m48s  (47.4%)38.0%  lr: 0.026386  loss: 0.001917  eta: 0h5m  tot: 0h4m48s  (47.6%)39.0%  lr: 0.026306  loss: 0.001904  eta: 0h4m  tot: 0h4m49s  (47.8%)39.2%  lr: 0.026266  loss: 0.001910  eta: 0h4m  tot: 0h4m50s  (47.8%)40.0%  lr: 0.026216  loss: 0.001909  eta: 0h4m  tot: 0h4m50s  (48.0%)40.3%  lr: 0.026186  loss: 0.001909  eta: 0h4m  tot: 0h4m51s  (48.1%)40.4%  lr: 0.026186  loss: 0.001914  eta: 0h4m  tot: 0h4m51s  (48.1%)41.2%  lr: 0.026126  loss: 0.001918  eta: 0h4m  tot: 0h4m52s  (48.2%)41.8%  lr: 0.026076  loss: 0.001924  eta: 0h4m  tot: 0h4m52s  (48.4%)42.0%  lr: 0.026036  loss: 0.001928  eta: 0h4m  tot: 0h4m53s  (48.4%)42.2%  lr: 0.025966  loss: 0.001935  eta: 0h4m  tot: 0h4m53s  (48.4%)42.5%  lr: 0.025936  loss: 0.001938  eta: 0h4m  tot: 0h4m53s  (48.5%)42.8%  lr: 0.025916  loss: 0.001939  eta: 0h4m  tot: 0h4m53s  (48.6%)43.2%  lr: 0.025876  loss: 0.001934  eta: 0h4m  tot: 0h4m54s  (48.6%)43.5%  lr: 0.025856  loss: 0.001936  eta: 0h4m  tot: 0h4m54s  (48.7%)44.0%  lr: 0.025776  loss: 0.001931  eta: 0h4m  tot: 0h4m55s  (48.8%)44.3%  lr: 0.025756  loss: 0.001943  eta: 0h4m  tot: 0h4m55s  (48.9%)45.0%  lr: 0.025666  loss: 0.001938  eta: 0h4m  tot: 0h4m56s  (49.0%)45.5%  lr: 0.025606  loss: 0.001947  eta: 0h4m  tot: 0h4m56s  (49.1%)0.001946  eta: 0h4m  tot: 0h4m56s  (49.1%)0.025586  loss: 0.001946  eta: 0h4m  tot: 0h4m57s  (49.1%)46.0%  lr: 0.025556  loss: 0.001945  eta: 0h4m  tot: 0h4m57s  (49.2%)46.1%  lr: 0.025506  loss: 0.001943  eta: 0h4m  tot: 0h4m57s  (49.2%)47.7%  lr: 0.025365  loss: 0.001941  eta: 0h4m  tot: 0h4m59s  (49.5%)48.6%  lr: 0.025285  loss: 0.001950  eta: 0h4m  tot: 0h5m1s  (49.7%)48.7%  lr: 0.025275  loss: 0.001951  eta: 0h4m  tot: 0h5m1s  (49.7%)49.7%  lr: 0.025185  loss: 0.001944  eta: 0h4m  tot: 0h5m2s  (49.9%)  loss: 0.001941  eta: 0h4m  tot: 0h5m2s  (50.0%)50.3%  lr: 0.025155  loss: 0.001940  eta: 0h4m  tot: 0h5m3s  (50.1%)50.9%  lr: 0.025085  loss: 0.001935  eta: 0h4m  tot: 0h5m3s  (50.2%)51.9%  lr: 0.025005  loss: 0.001933  eta: 0h4m  tot: 0h5m5s  (50.4%)52.1%  lr: 0.024975  loss: 0.001934  eta: 0h4m  tot: 0h5m5s  (50.4%)52.4%  lr: 0.024955  loss: 0.001931  eta: 0h4m  tot: 0h5m5s  (50.5%)52.7%  lr: 0.024915  loss: 0.001935  eta: 0h4m  tot: 0h5m5s  (50.5%)53.7%  lr: 0.024865  loss: 0.001937  eta: 0h4m  tot: 0h5m7s  (50.7%)54.6%  lr: 0.024775  loss: 0.001938  eta: 0h4m  tot: 0h5m8s  (50.9%)55.1%  lr: 0.024735  loss: 0.001937  eta: 0h4m  tot: 0h5m8s  (51.0%)55.5%  lr: 0.024725  loss: 0.001932  eta: 0h4m  tot: 0h5m8s  (51.1%)55.9%  lr: 0.024665  loss: 0.001936  eta: 0h4m  tot: 0h5m9s  (51.2%)56.2%  lr: 0.024655  loss: 0.001939  eta: 0h4m  tot: 0h5m9s  (51.2%)56.6%  lr: 0.024605  loss: 0.001940  eta: 0h4m  tot: 0h5m10s  (51.3%)56.7%  lr: 0.024605  loss: 0.001942  eta: 0h4m  tot: 0h5m10s  (51.3%)57.5%  lr: 0.024515  loss: 0.001940  eta: 0h4m  tot: 0h5m11s  (51.5%)57.7%  lr: 0.024485  loss: 0.001939  eta: 0h4m  tot: 0h5m11s  (51.5%)58.5%  lr: 0.024425  loss: 0.001945  eta: 0h4m  tot: 0h5m12s  (51.7%)  lr: 0.024264  loss: 0.001942  eta: 0h4m  tot: 0h5m13s  (51.9%)60.2%  lr: 0.024204  loss: 0.001947  eta: 0h4m  tot: 0h5m14s  (52.0%)61.0%  lr: 0.024114  loss: 0.001940  eta: 0h4m  tot: 0h5m15s  (52.2%)61.3%  lr: 0.024084  loss: 0.001941  eta: 0h4m  tot: 0h5m15s  (52.3%)61.7%  lr: 0.024014  loss: 0.001945  eta: 0h4m  tot: 0h5m16s  (52.3%)61.9%  lr: 0.024014  loss: 0.001947  eta: 0h4m  tot: 0h5m16s  (52.4%)64.9%  lr: 0.023754  loss: 0.001942  eta: 0h4m  tot: 0h5m19s  (53.0%)65.6%  lr: 0.023684  loss: 0.001944  eta: 0h4m  tot: 0h5m20s  (53.1%)66.0%  lr: 0.023634  loss: 0.001944  eta: 0h4m  tot: 0h5m21s  (53.2%)66.3%  lr: 0.023624  loss: 0.001941  eta: 0h4m  tot: 0h5m21s  (53.3%)%  lr: 0.023554  loss: 0.001948  eta: 0h4m  tot: 0h5m22s  (53.4%)66.9%  lr: 0.023544  loss: 0.001950  eta: 0h4m  tot: 0h5m22s  (53.4%)67.4%  lr: 0.023514  loss: 0.001951  eta: 0h4m  tot: 0h5m22s  (53.5%)67.6%  lr: 0.023514  loss: 0.001950  eta: 0h4m  tot: 0h5m23s  (53.5%)68.4%  lr: 0.023444  loss: 0.001950  eta: 0h4m  tot: 0h5m24s  (53.7%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.020371  loss: 0.001939  eta: 0h3m  tot: 0h5m59s  (60.0%)9.0%  lr: 0.023393  loss: 0.001958  eta: 0h4m  tot: 0h5m24s  (53.8%)69.9%  lr: 0.023303  loss: 0.001963  eta: 0h4m  tot: 0h5m25s  (54.0%)70.2%  lr: 0.023283  loss: 0.001962  eta: 0h4m  tot: 0h5m26s  (54.0%)71.0%  lr: 0.023153  loss: 0.001960  eta: 0h4m  tot: 0h5m26s  (54.2%)71.3%  lr: 0.023123  loss: 0.001958  eta: 0h4m  tot: 0h5m27s  (54.3%)75.0%  lr: 0.022733  loss: 0.001960  eta: 0h4m  tot: 0h5m31s  (55.0%)75.7%  lr: 0.022653  loss: 0.001963  eta: 0h4m  tot: 0h5m32s  (55.1%)75.9%  lr: 0.022633  loss: 0.001963  eta: 0h4m  tot: 0h5m32s  (55.2%)76.2%  lr: 0.022613  loss: 0.001963  eta: 0h4m  tot: 0h5m32s  (55.2%)76.7%  lr: 0.022603  loss: 0.001962  eta: 0h4m  tot: 0h5m33s  (55.3%)77.1%  lr: 0.022543  loss: 0.001961  eta: 0h4m  tot: 0h5m33s  (55.4%)77.7%  lr: 0.022473  loss: 0.001961  eta: 0h4m  tot: 0h5m34s  (55.5%)78.4%  lr: 0.022382  loss: 0.001960  eta: 0h4m  tot: 0h5m35s  (55.7%)78.5%  lr: 0.022372  loss: 0.001959  eta: 0h4m  tot: 0h5m35s  (55.7%)78.8%  lr: 0.022342  loss: 0.001961  eta: 0h4m  tot: 0h5m35s  (55.8%)78.9%  lr: 0.022342  loss: 0.001961  eta: 0h4m  tot: 0h5m35s  (55.8%)79.9%  lr: 0.022242  loss: 0.001954  eta: 0h4m  tot: 0h5m36s  (56.0%)%  lr: 0.022232  loss: 0.001957  eta: 0h4m  tot: 0h5m37s  (56.1%)82.3%  lr: 0.022092  loss: 0.001968  eta: 0h4m  tot: 0h5m39s  (56.5%)%  lr: 0.022092  loss: 0.001967  eta: 0h4m  tot: 0h5m39s  (56.5%)82.6%  lr: 0.022092  loss: 0.001969  eta: 0h4m  tot: 0h5m39s  (56.5%)82.9%  lr: 0.022072  loss: 0.001967  eta: 0h4m  tot: 0h5m40s  (56.6%)83.3%  lr: 0.022072  loss: 0.001968  eta: 0h4m  tot: 0h5m40s  (56.7%)83.5%  lr: 0.022052  loss: 0.001969  eta: 0h4m  tot: 0h5m40s  (56.7%)83.9%  lr: 0.021982  loss: 0.001966  eta: 0h4m  tot: 0h5m41s  (56.8%)84.4%  lr: 0.021922  loss: 0.001966  eta: 0h4m  tot: 0h5m42s  (56.9%)84.7%  lr: 0.021882  loss: 0.001966  eta: 0h4m  tot: 0h5m42s  (56.9%)85.2%  lr: 0.021832  loss: 0.001964  eta: 0h4m  tot: 0h5m42s  (57.0%)86.6%  lr: 0.021692  loss: 0.001958  eta: 0h4m  tot: 0h5m44s  (57.3%)87.5%  lr: 0.021622  loss: 0.001959  eta: 0h4m  tot: 0h5m45s  (57.5%)87.7%  lr: 0.021622  loss: 0.001958  eta: 0h4m  tot: 0h5m45s  (57.5%)88.4%  lr: 0.021562  loss: 0.001955  eta: 0h4m  tot: 0h5m46s  (57.7%) (57.8%)90.2%  lr: 0.021482  loss: 0.001955  eta: 0h4m  tot: 0h5m48s  (58.0%)90.7%  lr: 0.021412  loss: 0.001955  eta: 0h3m  tot: 0h5m49s  (58.1%)91.8%  lr: 0.021251  loss: 0.001949  eta: 0h3m  tot: 0h5m50s  (58.4%)92.1%  lr: 0.021181  loss: 0.001950  eta: 0h3m  tot: 0h5m50s  (58.4%)92.2%  lr: 0.021161  loss: 0.001950  eta: 0h3m  tot: 0h5m50s  (58.4%)92.6%  lr: 0.021151  loss: 0.001949  eta: 0h3m  tot: 0h5m51s  (58.5%)93.0%  lr: 0.021101  loss: 0.001948  eta: 0h3m  tot: 0h5m51s  (58.6%)93.4%  lr: 0.021081  loss: 0.001947  eta: 0h3m  tot: 0h5m51s  (58.7%)93.6%  lr: 0.021061  loss: 0.001947  eta: 0h3m  tot: 0h5m52s  (58.7%)94.0%  lr: 0.021011  loss: 0.001944  eta: 0h3m  tot: 0h5m52s  (58.8%)94.1%  lr: 0.021011  loss: 0.001944  eta: 0h3m  tot: 0h5m52s  (58.8%)94.7%  lr: 0.020951  loss: 0.001938  eta: 0h3m  tot: 0h5m53s  (58.9%)95.2%  lr: 0.020861  loss: 0.001937  eta: 0h3m  tot: 0h5m54s  (59.0%)95.3%  lr: 0.020851  loss: 0.001935  eta: 0h3m  tot: 0h5m54s  (59.1%)95.4%  lr: 0.020831  loss: 0.001935  eta: 0h3m  tot: 0h5m54s  (59.1%)97.3%  lr: 0.020591  loss: 0.001932  eta: 0h3m  tot: 0h5m56s  (59.5%)%  lr: 0.020581  loss: 0.001931  eta: 0h3m  tot: 0h5m56s  (59.5%)97.5%  lr: 0.020561  loss: 0.001931  eta: 0h3m  tot: 0h5m56s  (59.5%)98.0%  lr: 0.020521  loss: 0.001932  eta: 0h3m  tot: 0h5m57s  (59.6%)%  lr: 0.020481  loss: 0.001934  eta: 0h3m  tot: 0h5m58s  (59.7%)98.9%  lr: 0.020461  loss: 0.001936  eta: 0h3m  tot: 0h5m58s  (59.8%)\n",
      " ---+++                Epoch    2 Train error : 0.00191439 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54.8%  lr: 0.014885  loss: 0.001518  eta: 0h2m  tot: 0h7m4s  (71.0%))2%  lr: 0.019970  loss: 0.001468  eta: 0h6m  tot: 0h6m4s  (60.0%)0.3%  lr: 0.019950  loss: 0.001341  eta: 0h5m  tot: 0h6m4s  (60.1%)0.4%  lr: 0.019930  loss: 0.001523  eta: 0h5m  tot: 0h6m4s  (60.1%)0.7%  lr: 0.019910  loss: 0.001693  eta: 0h4m  tot: 0h6m5s  (60.1%)1.0%  lr: 0.019890  loss: 0.001739  eta: 0h4m  tot: 0h6m5s  (60.2%)1.1%  lr: 0.019890  loss: 0.001743  eta: 0h4m  tot: 0h6m5s  (60.2%)1.3%  lr: 0.019880  loss: 0.001839  eta: 0h4m  tot: 0h6m5s  (60.3%)2.7%  lr: 0.019750  loss: 0.001704  eta: 0h4m  tot: 0h6m7s  (60.5%)3.0%  lr: 0.019720  loss: 0.001696  eta: 0h4m  tot: 0h6m7s  (60.6%)3.1%  lr: 0.019710  loss: 0.001660  eta: 0h4m  tot: 0h6m7s  (60.6%)3.3%  lr: 0.019700  loss: 0.001663  eta: 0h4m  tot: 0h6m8s  (60.7%)3.5%  lr: 0.019700  loss: 0.001632  eta: 0h3m  tot: 0h6m8s  (60.7%)3.9%  lr: 0.019670  loss: 0.001631  eta: 0h3m  tot: 0h6m8s  (60.8%)4.1%  lr: 0.019670  loss: 0.001637  eta: 0h3m  tot: 0h6m8s  (60.8%)4.7%  lr: 0.019610  loss: 0.001598  eta: 0h3m  tot: 0h6m9s  (60.9%)5.0%  lr: 0.019580  loss: 0.001652  eta: 0h3m  tot: 0h6m9s  (61.0%)5.7%  lr: 0.019530  loss: 0.001650  eta: 0h3m  tot: 0h6m10s  (61.1%)5.9%  lr: 0.019520  loss: 0.001638  eta: 0h3m  tot: 0h6m10s  (61.2%)6.4%  lr: 0.019479  loss: 0.001602  eta: 0h3m  tot: 0h6m11s  (61.3%)6.6%  lr: 0.019449  loss: 0.001590  eta: 0h3m  tot: 0h6m11s  (61.3%)7.0%  lr: 0.019359  loss: 0.001610  eta: 0h3m  tot: 0h6m12s  (61.4%)7.7%  lr: 0.019329  loss: 0.001601  eta: 0h3m  tot: 0h6m12s  (61.5%)8.6%  lr: 0.019249  loss: 0.001573  eta: 0h3m  tot: 0h6m13s  (61.7%)8.7%  lr: 0.019219  loss: 0.001576  eta: 0h3m  tot: 0h6m13s  (61.7%)9.0%  lr: 0.019169  loss: 0.001563  eta: 0h3m  tot: 0h6m14s  (61.8%)11.0%  lr: 0.019069  loss: 0.001487  eta: 0h3m  tot: 0h6m16s  (62.2%)11.3%  lr: 0.019059  loss: 0.001475  eta: 0h3m  tot: 0h6m17s  (62.3%)11.5%  lr: 0.019029  loss: 0.001475  eta: 0h3m  tot: 0h6m17s  (62.3%)12.2%  lr: 0.018999  loss: 0.001477  eta: 0h3m  tot: 0h6m17s  (62.4%)12.9%  lr: 0.018959  loss: 0.001470  eta: 0h3m  tot: 0h6m18s  (62.6%)13.7%  lr: 0.018849  loss: 0.001509  eta: 0h3m  tot: 0h6m19s  (62.7%)  lr: 0.018809  loss: 0.001502  eta: 0h3m  tot: 0h6m20s  (62.8%)14.7%  lr: 0.018739  loss: 0.001508  eta: 0h3m  tot: 0h6m20s  (62.9%)14.9%  lr: 0.018739  loss: 0.001503  eta: 0h3m  tot: 0h6m20s  (63.0%)14.9%  lr: 0.018719  loss: 0.001503  eta: 0h3m  tot: 0h6m20s  (63.0%)15.3%  lr: 0.018639  loss: 0.001509  eta: 0h3m  tot: 0h6m21s  (63.1%)15.8%  lr: 0.018589  loss: 0.001505  eta: 0h3m  tot: 0h6m21s  (63.2%)16.7%  lr: 0.018529  loss: 0.001488  eta: 0h3m  tot: 0h6m22s  (63.3%)17.1%  lr: 0.018489  loss: 0.001491  eta: 0h3m  tot: 0h6m23s  (63.4%)17.7%  lr: 0.018418  loss: 0.001493  eta: 0h3m  tot: 0h6m23s  (63.5%)17.8%  lr: 0.018398  loss: 0.001496  eta: 0h3m  tot: 0h6m23s  (63.6%)18.3%  lr: 0.018378  loss: 0.001487  eta: 0h3m  tot: 0h6m24s  (63.7%)18.6%  lr: 0.018358  loss: 0.001487  eta: 0h3m  tot: 0h6m24s  (63.7%)18.7%  lr: 0.018338  loss: 0.001482  eta: 0h3m  tot: 0h6m24s  (63.7%)19.9%  lr: 0.018198  loss: 0.001489  eta: 0h3m  tot: 0h6m26s  (64.0%)20.2%  lr: 0.018198  loss: 0.001489  eta: 0h3m  tot: 0h6m26s  (64.0%)20.3%  lr: 0.018168  loss: 0.001494  eta: 0h3m  tot: 0h6m26s  (64.1%)21.0%  lr: 0.018138  loss: 0.001496  eta: 0h3m  tot: 0h6m27s  (64.2%)21.1%  lr: 0.018128  loss: 0.001500  eta: 0h3m  tot: 0h6m27s  (64.2%)21.2%  lr: 0.018128  loss: 0.001495  eta: 0h3m  tot: 0h6m27s  (64.2%)21.5%  lr: 0.018118  loss: 0.001490  eta: 0h3m  tot: 0h6m27s  (64.3%)21.8%  lr: 0.018098  loss: 0.001490  eta: 0h3m  tot: 0h6m28s  (64.4%)22.1%  lr: 0.018058  loss: 0.001485  eta: 0h3m  tot: 0h6m28s  (64.4%)22.2%  lr: 0.018008  loss: 0.001488  eta: 0h3m  tot: 0h6m28s  (64.4%)22.3%  lr: 0.017998  loss: 0.001489  eta: 0h3m  tot: 0h6m28s  (64.5%)  loss: 0.001477  eta: 0h3m  tot: 0h6m29s  (64.6%)24.0%  lr: 0.017818  loss: 0.001454  eta: 0h3m  tot: 0h6m30s  (64.8%)%  lr: 0.017738  loss: 0.001457  eta: 0h3m  tot: 0h6m31s  (64.9%)25.6%  lr: 0.017638  loss: 0.001472  eta: 0h3m  tot: 0h6m32s  (65.1%)25.8%  lr: 0.017618  loss: 0.001481  eta: 0h3m  tot: 0h6m32s  (65.2%)26.3%  lr: 0.017608  loss: 0.001481  eta: 0h3m  tot: 0h6m33s  (65.3%)27.2%  lr: 0.017528  loss: 0.001508  eta: 0h3m  tot: 0h6m34s  (65.4%)27.7%  lr: 0.017468  loss: 0.001507  eta: 0h3m  tot: 0h6m34s  (65.5%)27.8%  lr: 0.017468  loss: 0.001510  eta: 0h3m  tot: 0h6m35s  (65.6%)28.6%  lr: 0.017407  loss: 0.001516  eta: 0h3m  tot: 0h6m35s  (65.7%)29.2%  lr: 0.017357  loss: 0.001520  eta: 0h3m  tot: 0h6m36s  (65.8%)29.4%  lr: 0.017347  loss: 0.001519  eta: 0h3m  tot: 0h6m36s  (65.9%)29.5%  lr: 0.017317  loss: 0.001516  eta: 0h3m  tot: 0h6m36s  (65.9%)31.2%  lr: 0.017147  loss: 0.001514  eta: 0h3m  tot: 0h6m38s  (66.2%)31.5%  lr: 0.017147  loss: 0.001514  eta: 0h3m  tot: 0h6m38s  (66.3%)32.5%  lr: 0.017067  loss: 0.001536  eta: 0h3m  tot: 0h6m39s  (66.5%)32.6%  lr: 0.017057  loss: 0.001535  eta: 0h3m  tot: 0h6m39s  (66.5%)32.9%  lr: 0.017037  loss: 0.001541  eta: 0h3m  tot: 0h6m40s  (66.6%)33.2%  lr: 0.016997  loss: 0.001544  eta: 0h3m  tot: 0h6m40s  (66.6%)33.6%  lr: 0.016917  loss: 0.001540  eta: 0h3m  tot: 0h6m41s  (66.7%)34.3%  lr: 0.016857  loss: 0.001534  eta: 0h3m  tot: 0h6m42s  (66.9%)34.5%  lr: 0.016847  loss: 0.001535  eta: 0h3m  tot: 0h6m42s  (66.9%)34.6%  lr: 0.016847  loss: 0.001534  eta: 0h3m  tot: 0h6m42s  (66.9%)35.1%  lr: 0.016777  loss: 0.001528  eta: 0h3m  tot: 0h6m43s  (67.0%)%  lr: 0.016707  loss: 0.001524  eta: 0h3m  tot: 0h6m43s  (67.1%)35.6%  lr: 0.016657  loss: 0.001525  eta: 0h3m  tot: 0h6m43s  (67.1%)36.2%  lr: 0.016627  loss: 0.001523  eta: 0h3m  tot: 0h6m44s  (67.2%)%  lr: 0.016557  loss: 0.001519  eta: 0h3m  tot: 0h6m45s  (67.4%)37.0%  lr: 0.016547  loss: 0.001516  eta: 0h3m  tot: 0h6m45s  (67.4%)37.2%  lr: 0.016527  loss: 0.001515  eta: 0h3m  tot: 0h6m45s  (67.4%)37.8%  lr: 0.016467  loss: 0.001516  eta: 0h3m  tot: 0h6m46s  (67.6%)37.9%  lr: 0.016447  loss: 0.001518  eta: 0h3m  tot: 0h6m46s  (67.6%)38.2%  lr: 0.016447  loss: 0.001516  eta: 0h3m  tot: 0h6m46s  (67.6%)39.2%  lr: 0.016346  loss: 0.001512  eta: 0h2m  tot: 0h6m47s  (67.8%)40.3%  lr: 0.016246  loss: 0.001507  eta: 0h2m  tot: 0h6m48s  (68.1%)40.6%  lr: 0.016226  loss: 0.001508  eta: 0h2m  tot: 0h6m48s  (68.1%)41.1%  lr: 0.016216  loss: 0.001510  eta: 0h2m  tot: 0h6m49s  (68.2%)41.5%  lr: 0.016176  loss: 0.001511  eta: 0h2m  tot: 0h6m49s  (68.3%)42.1%  lr: 0.016106  loss: 0.001518  eta: 0h2m  tot: 0h6m50s  (68.4%)42.8%  lr: 0.016036  loss: 0.001520  eta: 0h2m  tot: 0h6m51s  (68.6%)42.9%  lr: 0.016006  loss: 0.001519  eta: 0h2m  tot: 0h6m51s  (68.6%)43.5%  lr: 0.015946  loss: 0.001514  eta: 0h2m  tot: 0h6m52s  (68.7%)%  lr: 0.015926  loss: 0.001513  eta: 0h2m  tot: 0h6m52s  (68.8%)43.9%  lr: 0.015926  loss: 0.001512  eta: 0h2m  tot: 0h6m52s  (68.8%)44.4%  lr: 0.015896  loss: 0.001513  eta: 0h2m  tot: 0h6m52s  (68.9%)44.7%  lr: 0.015856  loss: 0.001517  eta: 0h2m  tot: 0h6m53s  (68.9%)45.0%  lr: 0.015786  loss: 0.001517  eta: 0h2m  tot: 0h6m53s  (69.0%)45.1%  lr: 0.015786  loss: 0.001516  eta: 0h2m  tot: 0h6m53s  (69.0%)45.6%  lr: 0.015736  loss: 0.001514  eta: 0h2m  tot: 0h6m54s  (69.1%)46.0%  lr: 0.015736  loss: 0.001513  eta: 0h2m  tot: 0h6m54s  (69.2%)46.9%  lr: 0.015576  loss: 0.001512  eta: 0h2m  tot: 0h6m55s  (69.4%)47.3%  lr: 0.015556  loss: 0.001515  eta: 0h2m  tot: 0h6m56s  (69.5%)47.4%  lr: 0.015546  loss: 0.001513  eta: 0h2m  tot: 0h6m56s  (69.5%)49.1%  lr: 0.015486  loss: 0.001517  eta: 0h2m  tot: 0h6m58s  (69.8%)49.5%  lr: 0.015425  loss: 0.001513  eta: 0h2m  tot: 0h6m58s  (69.9%)49.9%  lr: 0.015415  loss: 0.001517  eta: 0h2m  tot: 0h6m58s  (70.0%)50.1%  lr: 0.015365  loss: 0.001515  eta: 0h2m  tot: 0h6m59s  (70.0%)50.6%  lr: 0.015315  loss: 0.001516  eta: 0h2m  tot: 0h6m59s  (70.1%)51.3%  lr: 0.015235  loss: 0.001512  eta: 0h2m  tot: 0h7m0s  (70.3%)51.6%  lr: 0.015225  loss: 0.001511  eta: 0h2m  tot: 0h7m1s  (70.3%)52.2%  lr: 0.015185  loss: 0.001509  eta: 0h2m  tot: 0h7m1s  (70.4%)53.0%  lr: 0.015075  loss: 0.001512  eta: 0h2m  tot: 0h7m2s  (70.6%)54.0%  lr: 0.014905  loss: 0.001514  eta: 0h2m  tot: 0h7m3s  (70.8%)54.9%  lr: 0.014855  loss: 0.001517  eta: 0h2m  tot: 0h7m4s  (71.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010391  loss: 0.001552  eta: 0h1m  tot: 0h7m57s  (80.0%).5%  lr: 0.014785  loss: 0.001527  eta: 0h2m  tot: 0h7m5s  (71.1%)55.7%  lr: 0.014775  loss: 0.001528  eta: 0h2m  tot: 0h7m5s  (71.1%)55.9%  lr: 0.014715  loss: 0.001530  eta: 0h2m  tot: 0h7m6s  (71.2%)56.4%  lr: 0.014645  loss: 0.001529  eta: 0h2m  tot: 0h7m6s  (71.3%)57.6%  lr: 0.014545  loss: 0.001540  eta: 0h2m  tot: 0h7m7s  (71.5%)57.7%  lr: 0.014545  loss: 0.001540  eta: 0h2m  tot: 0h7m8s  (71.5%)58.0%  lr: 0.014525  loss: 0.001538  eta: 0h2m  tot: 0h7m8s  (71.6%)58.1%  lr: 0.014525  loss: 0.001539  eta: 0h2m  tot: 0h7m8s  (71.6%)58.2%  lr: 0.014525  loss: 0.001540  eta: 0h2m  tot: 0h7m8s  (71.6%)58.5%  lr: 0.014485  loss: 0.001539  eta: 0h2m  tot: 0h7m9s  (71.7%)58.7%  lr: 0.014475  loss: 0.001537  eta: 0h2m  tot: 0h7m9s  (71.7%)59.1%  lr: 0.014445  loss: 0.001536  eta: 0h2m  tot: 0h7m9s  (71.8%)59.6%  lr: 0.014425  loss: 0.001541  eta: 0h2m  tot: 0h7m10s  (71.9%)60.9%  lr: 0.014294  loss: 0.001541  eta: 0h2m  tot: 0h7m12s  (72.2%)61.1%  lr: 0.014294  loss: 0.001542  eta: 0h2m  tot: 0h7m12s  (72.2%)61.2%  lr: 0.014284  loss: 0.001542  eta: 0h2m  tot: 0h7m12s  (72.2%)61.4%  lr: 0.014264  loss: 0.001542  eta: 0h2m  tot: 0h7m12s  (72.3%)61.7%  lr: 0.014244  loss: 0.001543  eta: 0h2m  tot: 0h7m13s  (72.3%)61.8%  lr: 0.014224  loss: 0.001543  eta: 0h2m  tot: 0h7m13s  (72.4%)62.7%  lr: 0.014124  loss: 0.001547  eta: 0h2m  tot: 0h7m14s  (72.5%)62.8%  lr: 0.014094  loss: 0.001546  eta: 0h2m  tot: 0h7m14s  (72.6%)64.5%  lr: 0.013994  loss: 0.001542  eta: 0h2m  tot: 0h7m16s  (72.9%)64.6%  lr: 0.013994  loss: 0.001544  eta: 0h2m  tot: 0h7m16s  (72.9%)65.2%  lr: 0.013944  loss: 0.001547  eta: 0h2m  tot: 0h7m17s  (73.0%)65.3%  lr: 0.013944  loss: 0.001547  eta: 0h2m  tot: 0h7m17s  (73.1%)65.4%  lr: 0.013924  loss: 0.001545  eta: 0h2m  tot: 0h7m17s  (73.1%)65.9%  lr: 0.013864  loss: 0.001547  eta: 0h2m  tot: 0h7m18s  (73.2%)%  lr: 0.013744  loss: 0.001547  eta: 0h2m  tot: 0h7m19s  (73.3%)66.8%  lr: 0.013734  loss: 0.001547  eta: 0h2m  tot: 0h7m19s  (73.4%)67.0%  lr: 0.013734  loss: 0.001546  eta: 0h2m  tot: 0h7m19s  (73.4%)67.4%  lr: 0.013714  loss: 0.001545  eta: 0h2m  tot: 0h7m19s  (73.5%)68.1%  lr: 0.013644  loss: 0.001550  eta: 0h2m  tot: 0h7m20s  (73.6%)68.2%  lr: 0.013624  loss: 0.001550  eta: 0h2m  tot: 0h7m20s  (73.6%)69.1%  lr: 0.013534  loss: 0.001548  eta: 0h2m  tot: 0h7m21s  (73.8%)69.2%  lr: 0.013534  loss: 0.001548  eta: 0h2m  tot: 0h7m22s  (73.8%)69.4%  lr: 0.013514  loss: 0.001550  eta: 0h2m  tot: 0h7m22s  (73.9%)69.6%  lr: 0.013494  loss: 0.001549  eta: 0h2m  tot: 0h7m22s  (73.9%)69.7%  lr: 0.013494  loss: 0.001549  eta: 0h2m  tot: 0h7m22s  (73.9%)70.5%  lr: 0.013424  loss: 0.001550  eta: 0h2m  tot: 0h7m23s  (74.1%)70.8%  lr: 0.013343  loss: 0.001549  eta: 0h2m  tot: 0h7m24s  (74.2%)70.9%  lr: 0.013333  loss: 0.001549  eta: 0h2m  tot: 0h7m24s  (74.2%)71.7%  lr: 0.013283  loss: 0.001545  eta: 0h2m  tot: 0h7m25s  (74.3%)72.7%  lr: 0.013193  loss: 0.001544  eta: 0h2m  tot: 0h7m26s  (74.5%)%)73.4%  lr: 0.013103  loss: 0.001544  eta: 0h2m  tot: 0h7m27s  (74.7%)73.5%  lr: 0.013073  loss: 0.001544  eta: 0h2m  tot: 0h7m27s  (74.7%)73.7%  lr: 0.013073  loss: 0.001544  eta: 0h2m  tot: 0h7m27s  (74.7%)74.0%  lr: 0.013053  loss: 0.001543  eta: 0h2m  tot: 0h7m27s  (74.8%)74.2%  lr: 0.013053  loss: 0.001542  eta: 0h2m  tot: 0h7m28s  (74.8%)74.7%  lr: 0.012993  loss: 0.001546  eta: 0h2m  tot: 0h7m28s  (74.9%)75.2%  lr: 0.012913  loss: 0.001545  eta: 0h2m  tot: 0h7m29s  (75.0%)76.7%  lr: 0.012753  loss: 0.001548  eta: 0h2m  tot: 0h7m30s  (75.3%)77.1%  lr: 0.012673  loss: 0.001545  eta: 0h2m  tot: 0h7m31s  (75.4%)77.4%  lr: 0.012643  loss: 0.001545  eta: 0h2m  tot: 0h7m31s  (75.5%)77.7%  lr: 0.012603  loss: 0.001546  eta: 0h2m  tot: 0h7m31s  (75.5%)78.0%  lr: 0.012553  loss: 0.001549  eta: 0h2m  tot: 0h7m32s  (75.6%)79.8%  lr: 0.012362  loss: 0.001550  eta: 0h2m  tot: 0h7m34s  (76.0%)80.5%  lr: 0.012262  loss: 0.001552  eta: 0h2m  tot: 0h7m35s  (76.1%)80.8%  lr: 0.012252  loss: 0.001551  eta: 0h2m  tot: 0h7m35s  (76.2%)80.8%  lr: 0.012252  loss: 0.001551  eta: 0h2m  tot: 0h7m35s  (76.2%)81.2%  lr: 0.012242  loss: 0.001552  eta: 0h2m  tot: 0h7m36s  (76.2%)83.3%  lr: 0.011992  loss: 0.001556  eta: 0h2m  tot: 0h7m38s  (76.7%)83.7%  lr: 0.011962  loss: 0.001554  eta: 0h2m  tot: 0h7m39s  (76.7%)83.8%  lr: 0.011942  loss: 0.001556  eta: 0h2m  tot: 0h7m39s  (76.8%)84.4%  lr: 0.011882  loss: 0.001554  eta: 0h2m  tot: 0h7m39s  (76.9%)85.1%  lr: 0.011832  loss: 0.001558  eta: 0h2m  tot: 0h7m40s  (77.0%)85.6%  lr: 0.011792  loss: 0.001558  eta: 0h2m  tot: 0h7m41s  (77.1%)85.7%  lr: 0.011772  loss: 0.001557  eta: 0h2m  tot: 0h7m41s  (77.1%)86.2%  lr: 0.011732  loss: 0.001558  eta: 0h2m  tot: 0h7m41s  (77.2%)87.3%  lr: 0.011602  loss: 0.001556  eta: 0h2m  tot: 0h7m43s  (77.5%)88.0%  lr: 0.011512  loss: 0.001558  eta: 0h2m  tot: 0h7m43s  (77.6%)88.4%  lr: 0.011472  loss: 0.001559  eta: 0h2m  tot: 0h7m44s  (77.7%)90.1%  lr: 0.011361  loss: 0.001558  eta: 0h2m  tot: 0h7m45s  (78.0%)90.7%  lr: 0.011261  loss: 0.001558  eta: 0h2m  tot: 0h7m46s  (78.1%)91.0%  lr: 0.011251  loss: 0.001556  eta: 0h2m  tot: 0h7m47s  (78.2%)91.8%  lr: 0.011161  loss: 0.001555  eta: 0h2m  tot: 0h7m48s  (78.4%)92.8%  lr: 0.011061  loss: 0.001556  eta: 0h2m  tot: 0h7m49s  (78.6%)93.2%  lr: 0.010991  loss: 0.001556  eta: 0h2m  tot: 0h7m49s  (78.6%)93.4%  lr: 0.010991  loss: 0.001555  eta: 0h2m  tot: 0h7m50s  (78.7%)94.4%  lr: 0.010921  loss: 0.001553  eta: 0h1m  tot: 0h7m51s  (78.9%)94.7%  lr: 0.010911  loss: 0.001553  eta: 0h1m  tot: 0h7m51s  (78.9%)94.8%  lr: 0.010901  loss: 0.001553  eta: 0h1m  tot: 0h7m51s  (79.0%)94.9%  lr: 0.010891  loss: 0.001554  eta: 0h1m  tot: 0h7m51s  (79.0%)96.2%  lr: 0.010811  loss: 0.001551  eta: 0h1m  tot: 0h7m53s  (79.2%)96.5%  lr: 0.010791  loss: 0.001553  eta: 0h1m  tot: 0h7m53s  (79.3%)97.6%  lr: 0.010661  loss: 0.001552  eta: 0h1m  tot: 0h7m54s  (79.5%)98.5%  lr: 0.010551  loss: 0.001551  eta: 0h1m  tot: 0h7m55s  (79.7%)98.8%  lr: 0.010531  loss: 0.001554  eta: 0h1m  tot: 0h7m56s  (79.8%)98.9%  lr: 0.010511  loss: 0.001555  eta: 0h1m  tot: 0h7m56s  (79.8%)\n",
      " ---+++                Epoch    3 Train error : 0.00155979 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65.1%  lr: 0.003924  loss: 0.001361  eta: <1min   tot: 0h9m15s  (93.0%)4%  lr: 0.009960  loss: 0.001533  eta: 0h1m  tot: 0h8m2s  (80.1%)0.6%  lr: 0.009950  loss: 0.001303  eta: 0h1m  tot: 0h8m2s  (80.1%)1.2%  lr: 0.009900  loss: 0.001273  eta: 0h2m  tot: 0h8m3s  (80.2%)1.3%  lr: 0.009890  loss: 0.001155  eta: 0h2m  tot: 0h8m3s  (80.3%)1.9%  lr: 0.009800  loss: 0.001159  eta: 0h2m  tot: 0h8m4s  (80.4%)2.2%  lr: 0.009780  loss: 0.001235  eta: 0h1m  tot: 0h8m4s  (80.4%)3.2%  lr: 0.009660  loss: 0.001266  eta: 0h1m  tot: 0h8m5s  (80.6%)4.1%  lr: 0.009620  loss: 0.001310  eta: 0h1m  tot: 0h8m6s  (80.8%)4.2%  lr: 0.009610  loss: 0.001295  eta: 0h1m  tot: 0h8m7s  (80.8%)4.5%  lr: 0.009590  loss: 0.001301  eta: 0h1m  tot: 0h8m7s  (80.9%)4.7%  lr: 0.009580  loss: 0.001288  eta: 0h1m  tot: 0h8m7s  (80.9%)4.8%  lr: 0.009550  loss: 0.001280  eta: 0h1m  tot: 0h8m7s  (81.0%)5.2%  lr: 0.009479  loss: 0.001271  eta: 0h1m  tot: 0h8m8s  (81.0%)5.9%  lr: 0.009449  loss: 0.001280  eta: 0h1m  tot: 0h8m8s  (81.2%)7.8%  lr: 0.009219  loss: 0.001320  eta: 0h1m  tot: 0h8m10s  (81.6%)8.4%  lr: 0.009179  loss: 0.001342  eta: 0h1m  tot: 0h8m11s  (81.7%)8.6%  lr: 0.009169  loss: 0.001333  eta: 0h1m  tot: 0h8m11s  (81.7%)9.3%  lr: 0.009089  loss: 0.001345  eta: 0h1m  tot: 0h8m12s  (81.9%)9.5%  lr: 0.009079  loss: 0.001330  eta: 0h1m  tot: 0h8m12s  (81.9%)10.1%  lr: 0.009009  loss: 0.001321  eta: 0h1m  tot: 0h8m13s  (82.0%)10.2%  lr: 0.008999  loss: 0.001321  eta: 0h1m  tot: 0h8m13s  (82.0%)10.4%  lr: 0.008969  loss: 0.001322  eta: 0h1m  tot: 0h8m13s  (82.1%)10.4%  lr: 0.008969  loss: 0.001327  eta: 0h1m  tot: 0h8m13s  (82.1%)10.8%  lr: 0.008919  loss: 0.001328  eta: 0h1m  tot: 0h8m14s  (82.2%)11.3%  lr: 0.008879  loss: 0.001313  eta: 0h1m  tot: 0h8m14s  (82.3%)11.5%  lr: 0.008879  loss: 0.001304  eta: 0h1m  tot: 0h8m15s  (82.3%)12.4%  lr: 0.008809  loss: 0.001325  eta: 0h1m  tot: 0h8m16s  (82.5%)13.1%  lr: 0.008759  loss: 0.001318  eta: 0h1m  tot: 0h8m16s  (82.6%)13.3%  lr: 0.008709  loss: 0.001315  eta: 0h1m  tot: 0h8m17s  (82.7%)14.0%  lr: 0.008619  loss: 0.001309  eta: 0h1m  tot: 0h8m17s  (82.8%)14.5%  lr: 0.008579  loss: 0.001314  eta: 0h1m  tot: 0h8m18s  (82.9%)15.1%  lr: 0.008539  loss: 0.001316  eta: 0h1m  tot: 0h8m19s  (83.0%)15.7%  lr: 0.008479  loss: 0.001317  eta: 0h1m  tot: 0h8m19s  (83.1%)17.1%  lr: 0.008318  loss: 0.001296  eta: 0h1m  tot: 0h8m20s  (83.4%)%  lr: 0.008278  loss: 0.001284  eta: 0h1m  tot: 0h8m21s  (83.5%)18.5%  lr: 0.008168  loss: 0.001292  eta: 0h1m  tot: 0h8m22s  (83.7%)19.4%  lr: 0.008118  loss: 0.001301  eta: 0h1m  tot: 0h8m23s  (83.9%)19.9%  lr: 0.008098  loss: 0.001296  eta: 0h1m  tot: 0h8m23s  (84.0%)20.1%  lr: 0.008098  loss: 0.001301  eta: 0h1m  tot: 0h8m24s  (84.0%)20.9%  lr: 0.007978  loss: 0.001305  eta: 0h1m  tot: 0h8m24s  (84.2%)21.1%  lr: 0.007978  loss: 0.001308  eta: 0h1m  tot: 0h8m25s  (84.2%)22.0%  lr: 0.007898  loss: 0.001323  eta: 0h1m  tot: 0h8m26s  (84.4%)22.6%  lr: 0.007868  loss: 0.001324  eta: 0h1m  tot: 0h8m26s  (84.5%)23.6%  lr: 0.007828  loss: 0.001319  eta: 0h1m  tot: 0h8m27s  (84.7%)24.2%  lr: 0.007798  loss: 0.001307  eta: 0h1m  tot: 0h8m28s  (84.8%)24.5%  lr: 0.007758  loss: 0.001306  eta: 0h1m  tot: 0h8m28s  (84.9%)25.0%  lr: 0.007688  loss: 0.001313  eta: 0h1m  tot: 0h8m29s  (85.0%)25.1%  lr: 0.007688  loss: 0.001312  eta: 0h1m  tot: 0h8m29s  (85.0%)25.2%  lr: 0.007688  loss: 0.001310  eta: 0h1m  tot: 0h8m29s  (85.0%)25.7%  lr: 0.007628  loss: 0.001312  eta: 0h1m  tot: 0h8m29s  (85.1%)25.8%  lr: 0.007588  loss: 0.001316  eta: 0h1m  tot: 0h8m30s  (85.2%)0h8m30s  (85.2%)26.4%  lr: 0.007508  loss: 0.001320  eta: 0h1m  tot: 0h8m30s  (85.3%)26.5%  lr: 0.007508  loss: 0.001318  eta: 0h1m  tot: 0h8m31s  (85.3%)h8m31s  (85.3%)27.5%  lr: 0.007407  loss: 0.001315  eta: 0h1m  tot: 0h8m32s  (85.5%)27.7%  lr: 0.007367  loss: 0.001313  eta: 0h1m  tot: 0h8m32s  (85.5%)27.8%  lr: 0.007367  loss: 0.001312  eta: 0h1m  tot: 0h8m32s  (85.6%)27.9%  lr: 0.007367  loss: 0.001311  eta: 0h1m  tot: 0h8m32s  (85.6%)28.1%  lr: 0.007337  loss: 0.001308  eta: 0h1m  tot: 0h8m33s  (85.6%)29.6%  lr: 0.007207  loss: 0.001311  eta: 0h1m  tot: 0h8m34s  (85.9%)30.8%  lr: 0.007007  loss: 0.001339  eta: 0h1m  tot: 0h8m36s  (86.2%)32.0%  lr: 0.006867  loss: 0.001346  eta: 0h1m  tot: 0h8m37s  (86.4%)86.5%)32.6%  lr: 0.006827  loss: 0.001347  eta: 0h1m  tot: 0h8m38s  (86.5%)33.6%  lr: 0.006757  loss: 0.001343  eta: 0h1m  tot: 0h8m39s  (86.7%)34.5%  lr: 0.006657  loss: 0.001343  eta: 0h1m  tot: 0h8m40s  (86.9%)35.0%  lr: 0.006627  loss: 0.001339  eta: 0h1m  tot: 0h8m41s  (87.0%)35.7%  lr: 0.006577  loss: 0.001339  eta: 0h1m  tot: 0h8m41s  (87.1%)36.0%  lr: 0.006577  loss: 0.001340  eta: 0h1m  tot: 0h8m42s  (87.2%)36.6%  lr: 0.006497  loss: 0.001334  eta: 0h1m  tot: 0h8m42s  (87.3%)37.4%  lr: 0.006416  loss: 0.001332  eta: 0h1m  tot: 0h8m43s  (87.5%)38.8%  lr: 0.006296  loss: 0.001337  eta: 0h1m  tot: 0h8m45s  (87.8%)39.2%  lr: 0.006276  loss: 0.001338  eta: 0h1m  tot: 0h8m45s  (87.8%)39.4%  lr: 0.006266  loss: 0.001337  eta: 0h1m  tot: 0h8m45s  (87.9%)39.5%  lr: 0.006256  loss: 0.001337  eta: 0h1m  tot: 0h8m45s  (87.9%)39.8%  lr: 0.006236  loss: 0.001335  eta: 0h1m  tot: 0h8m46s  (88.0%)40.3%  lr: 0.006196  loss: 0.001332  eta: 0h1m  tot: 0h8m46s  (88.1%)40.8%  lr: 0.006106  loss: 0.001333  eta: 0h1m  tot: 0h8m47s  (88.2%)41.7%  lr: 0.006026  loss: 0.001335  eta: 0h1m  tot: 0h8m48s  (88.3%)42.8%  lr: 0.005956  loss: 0.001335  eta: 0h1m  tot: 0h8m49s  (88.6%)43.4%  lr: 0.005906  loss: 0.001336  eta: 0h1m  tot: 0h8m50s  (88.7%)43.8%  lr: 0.005846  loss: 0.001336  eta: 0h1m  tot: 0h8m50s  (88.8%)44.6%  lr: 0.005796  loss: 0.001335  eta: 0h1m  tot: 0h8m51s  (88.9%)44.8%  lr: 0.005796  loss: 0.001331  eta: 0h1m  tot: 0h8m51s  (89.0%)46.4%  lr: 0.005706  loss: 0.001330  eta: <1min   tot: 0h8m53s  (89.3%)46.5%  lr: 0.005686  loss: 0.001333  eta: <1min   tot: 0h8m53s  (89.3%)%  lr: 0.005476  loss: 0.001353  eta: <1min   tot: 0h8m56s  (89.8%)49.1%  lr: 0.005466  loss: 0.001353  eta: <1min   tot: 0h8m56s  (89.8%)49.2%  lr: 0.005446  loss: 0.001352  eta: <1min   tot: 0h8m56s  (89.8%)49.7%  lr: 0.005365  loss: 0.001348  eta: <1min   tot: 0h8m57s  (89.9%)49.9%  lr: 0.005315  loss: 0.001348  eta: <1min   tot: 0h8m57s  (90.0%)50.1%  lr: 0.005285  loss: 0.001348  eta: <1min   tot: 0h8m57s  (90.0%)50.5%  lr: 0.005245  loss: 0.001344  eta: <1min   tot: 0h8m58s  (90.1%)50.8%  lr: 0.005235  loss: 0.001344  eta: <1min   tot: 0h8m58s  (90.2%)50.9%  lr: 0.005205  loss: 0.001347  eta: <1min   tot: 0h8m58s  (90.2%)51.4%  lr: 0.005165  loss: 0.001348  eta: <1min   tot: 0h8m59s  (90.3%)52.2%  lr: 0.005075  loss: 0.001347  eta: <1min   tot: 0h9m0s  (90.4%)52.8%  lr: 0.005035  loss: 0.001351  eta: <1min   tot: 0h9m1s  (90.6%)53.3%  lr: 0.004995  loss: 0.001350  eta: <1min   tot: 0h9m1s  (90.7%)54.6%  lr: 0.004875  loss: 0.001353  eta: <1min   tot: 0h9m2s  (90.9%)h9m3s  (91.0%)55.8%  lr: 0.004765  loss: 0.001356  eta: <1min   tot: 0h9m4s  (91.2%)56.0%  lr: 0.004765  loss: 0.001358  eta: <1min   tot: 0h9m4s  (91.2%)56.4%  lr: 0.004745  loss: 0.001364  eta: <1min   tot: 0h9m4s  (91.3%)56.6%  lr: 0.004695  loss: 0.001365  eta: <1min   tot: 0h9m5s  (91.3%)57.6%  lr: 0.004605  loss: 0.001354  eta: <1min   tot: 0h9m6s  (91.5%)57.7%  lr: 0.004595  loss: 0.001353  eta: <1min   tot: 0h9m6s  (91.5%)59.1%  lr: 0.004445  loss: 0.001356  eta: <1min   tot: 0h9m8s  (91.8%)59.6%  lr: 0.004435  loss: 0.001358  eta: <1min   tot: 0h9m8s  (91.9%)59.9%  lr: 0.004425  loss: 0.001358  eta: <1min   tot: 0h9m8s  (92.0%)60.1%  lr: 0.004404  loss: 0.001356  eta: <1min   tot: 0h9m9s  (92.0%)60.5%  lr: 0.004334  loss: 0.001362  eta: <1min   tot: 0h9m9s  (92.1%)60.9%  lr: 0.004264  loss: 0.001360  eta: <1min   tot: 0h9m10s  (92.2%)62.1%  lr: 0.004154  loss: 0.001364  eta: <1min   tot: 0h9m11s  (92.4%)62.5%  lr: 0.004124  loss: 0.001363  eta: <1min   tot: 0h9m12s  (92.5%)63.5%  lr: 0.004024  loss: 0.001361  eta: <1min   tot: 0h9m13s  (92.7%)64.0%  lr: 0.003964  loss: 0.001363  eta: <1min   tot: 0h9m13s  (92.8%)64.4%  lr: 0.003954  loss: 0.001365  eta: <1min   tot: 0h9m14s  (92.9%)64.6%  lr: 0.003944  loss: 0.001363  eta: <1min   tot: 0h9m14s  (92.9%)65.2%  lr: 0.003894  loss: 0.001360  eta: <1min   tot: 0h9m15s  (93.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99.9%  lr: 0.000310  loss: 0.001364  eta: <1min   tot: 0h9m55s  (100.0%)5.4%  lr: 0.003894  loss: 0.001360  eta: <1min   tot: 0h9m15s  (93.1%)65.4%  lr: 0.003894  loss: 0.001360  eta: <1min   tot: 0h9m15s  (93.1%)66.0%  lr: 0.003824  loss: 0.001359  eta: <1min   tot: 0h9m16s  (93.2%)66.7%  lr: 0.003734  loss: 0.001359  eta: <1min   tot: 0h9m17s  (93.3%)%  lr: 0.003734  loss: 0.001360  eta: <1min   tot: 0h9m17s  (93.4%)67.6%  lr: 0.003644  loss: 0.001361  eta: <1min   tot: 0h9m18s  (93.5%)68.8%  lr: 0.003544  loss: 0.001360  eta: <1min   tot: 0h9m19s  (93.8%)68.9%  lr: 0.003544  loss: 0.001360  eta: <1min   tot: 0h9m19s  (93.8%)69.4%  lr: 0.003504  loss: 0.001356  eta: <1min   tot: 0h9m20s  (93.9%)69.8%  lr: 0.003434  loss: 0.001360  eta: <1min   tot: 0h9m21s  (94.0%)70.5%  lr: 0.003303  loss: 0.001356  eta: <1min   tot: 0h9m22s  (94.1%)71.7%  lr: 0.003113  loss: 0.001354  eta: <1min   tot: 0h9m23s  (94.3%)72.0%  lr: 0.003093  loss: 0.001353  eta: <1min   tot: 0h9m23s  (94.4%)72.1%  lr: 0.003093  loss: 0.001352  eta: <1min   tot: 0h9m23s  (94.4%)72.2%  lr: 0.003063  loss: 0.001355  eta: <1min   tot: 0h9m24s  (94.4%)72.3%  lr: 0.003053  loss: 0.001354  eta: <1min   tot: 0h9m24s  (94.5%)73.3%  lr: 0.002943  loss: 0.001357  eta: <1min   tot: 0h9m25s  (94.7%)73.8%  lr: 0.002893  loss: 0.001356  eta: <1min   tot: 0h9m25s  (94.8%)%  lr: 0.002873  loss: 0.001356  eta: <1min   tot: 0h9m25s  (94.8%)74.2%  lr: 0.002853  loss: 0.001355  eta: <1min   tot: 0h9m26s  (94.8%)76.2%  lr: 0.002663  loss: 0.001353  eta: <1min   tot: 0h9m28s  (95.2%)76.5%  lr: 0.002663  loss: 0.001355  eta: <1min   tot: 0h9m29s  (95.3%)76.8%  lr: 0.002633  loss: 0.001357  eta: <1min   tot: 0h9m29s  (95.4%)77.2%  lr: 0.002593  loss: 0.001356  eta: <1min   tot: 0h9m29s  (95.4%)%  lr: 0.002583  loss: 0.001359  eta: <1min   tot: 0h9m30s  (95.5%)77.6%  lr: 0.002583  loss: 0.001358  eta: <1min   tot: 0h9m30s  (95.5%)78.6%  lr: 0.002493  loss: 0.001358  eta: <1min   tot: 0h9m31s  (95.7%)78.9%  lr: 0.002433  loss: 0.001358  eta: <1min   tot: 0h9m31s  (95.8%)80.1%  lr: 0.002312  loss: 0.001360  eta: <1min   tot: 0h9m33s  (96.0%)80.4%  lr: 0.002272  loss: 0.001360  eta: <1min   tot: 0h9m33s  (96.1%)80.8%  lr: 0.002222  loss: 0.001362  eta: <1min   tot: 0h9m33s  (96.2%)82.2%  lr: 0.002092  loss: 0.001364  eta: <1min   tot: 0h9m35s  (96.4%)82.3%  lr: 0.002092  loss: 0.001363  eta: <1min   tot: 0h9m35s  (96.5%)82.9%  lr: 0.002042  loss: 0.001361  eta: <1min   tot: 0h9m36s  (96.6%)83.4%  lr: 0.001962  loss: 0.001363  eta: <1min   tot: 0h9m36s  (96.7%)84.5%  lr: 0.001902  loss: 0.001360  eta: <1min   tot: 0h9m37s  (96.9%)84.6%  lr: 0.001882  loss: 0.001360  eta: <1min   tot: 0h9m37s  (96.9%)85.9%  lr: 0.001752  loss: 0.001361  eta: <1min   tot: 0h9m39s  (97.2%)86.2%  lr: 0.001732  loss: 0.001360  eta: <1min   tot: 0h9m39s  (97.2%)86.6%  lr: 0.001692  loss: 0.001362  eta: <1min   tot: 0h9m40s  (97.3%)87.0%  lr: 0.001662  loss: 0.001362  eta: <1min   tot: 0h9m40s  (97.4%)87.1%  lr: 0.001642  loss: 0.001362  eta: <1min   tot: 0h9m40s  (97.4%)87.3%  lr: 0.001602  loss: 0.001362  eta: <1min   tot: 0h9m40s  (97.5%)87.5%  lr: 0.001592  loss: 0.001362  eta: <1min   tot: 0h9m41s  (97.5%)87.8%  lr: 0.001572  loss: 0.001361  eta: <1min   tot: 0h9m41s  (97.6%)88.8%  lr: 0.001512  loss: 0.001360  eta: <1min   tot: 0h9m42s  (97.8%)89.1%  lr: 0.001452  loss: 0.001360  eta: <1min   tot: 0h9m43s  (97.8%)90.7%  lr: 0.001331  loss: 0.001359  eta: <1min   tot: 0h9m44s  (98.1%)91.4%  lr: 0.001221  loss: 0.001358  eta: <1min   tot: 0h9m45s  (98.3%)92.5%  lr: 0.001081  loss: 0.001356  eta: <1min   tot: 0h9m47s  (98.5%)93.4%  lr: 0.001001  loss: 0.001359  eta: <1min   tot: 0h9m47s  (98.7%)93.5%  lr: 0.000961  loss: 0.001359  eta: <1min   tot: 0h9m48s  (98.7%)94.0%  lr: 0.000891  loss: 0.001360  eta: <1min   tot: 0h9m48s  (98.8%)94.3%  lr: 0.000881  loss: 0.001360  eta: <1min   tot: 0h9m48s  (98.9%)95.0%  lr: 0.000811  loss: 0.001360  eta: <1min   tot: 0h9m49s  (99.0%)95.4%  lr: 0.000761  loss: 0.001359  eta: <1min   tot: 0h9m50s  (99.1%)95.5%  lr: 0.000751  loss: 0.001359  eta: <1min   tot: 0h9m50s  (99.1%)97.3%  lr: 0.000571  loss: 0.001359  eta: <1min   tot: 0h9m52s  (99.5%)99.2%  lr: 0.000421  loss: 0.001363  eta: <1min   tot: 0h9m54s  (99.8%)99.6%  lr: 0.000360  loss: 0.001364  eta: <1min   tot: 0h9m54s  (99.9%)99.7%  lr: 0.000320  loss: 0.001363  eta: <1min   tot: 0h9m54s  (99.9%)99.7%  lr: 0.000320  loss: 0.001363  eta: <1min   tot: 0h9m54s  (99.9%)99.8%  lr: 0.000320  loss: 0.001364  eta: <1min   tot: 0h9m55s  (100.0%)100.0%  lr: 0.000310  loss: 0.001364  eta: <1min   tot: 0h9m55s  (100.0%)\n",
      " ---+++                Epoch    4 Train error : 0.00137217 +++--- ���\n",
      "Saving model to file : sodd\n",
      "Saving model in tsv format : sodd.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!Starspace/starspace train -trainFile './data/tp_train.tsv' -model \"sodd\" -trainMode 3 \\\n",
    "    -adagrad true -ngrams 1 -epoch 5 -dim 100 -minCount 2 -verbose true -fileFormat \"labelDoc\" \\\n",
    "    -negSearchLimit 10 -similarity \"cosine\" -lr 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = {}\n",
    "for line in open('./sodd.tsv'):\n",
    "    word,*vec = line.strip().split()\n",
    "    vf = []\n",
    "    for v in vec:\n",
    "        # print(v)\n",
    "        vf.append(float(v))\n",
    "    starspace_embeddings[word] = np.array(vf)\n",
    "######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.521 | Hits@   1: 0.521\n",
      "DCG@   5: 0.615 | Hits@   5: 0.697\n",
      "DCG@  10: 0.634 | Hits@  10: 0.754\n",
      "DCG@ 100: 0.666 | Hits@ 100: 0.906\n",
      "DCG@ 500: 0.675 | Hits@ 500: 0.981\n",
      "DCG@1000: 0.678 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 91\t11\t68\t53\t25\t83\t34\t84\t95\t79\t24\t37\t39\t48\t96\t86\t12\t42\t47\t98\t19\t28\t8\t78\t100\t62\t97\t49\t69\t5\t71\t59\t82\t29...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = './data/tp_t.tsv' ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891266\n",
      "-0.0287272129208\n",
      "0.0460561104119\n",
      "0.0852593332529\n",
      "0.0243055559695\n",
      "-0.0729031041265\n",
      "0.0...\n",
      "Task HitsCount: 0.9999999900000002\n",
      "0.4999999975\n",
      "0.999999995\n",
      "0.4999999975\n",
      "0.999999995\n",
      "0.3333333322222222\n",
      "0.6666666644...\n",
      "Task DCGScore: 0.99999999\n",
      "0.4999999975\n",
      "0.815464872708\n",
      "0.4999999975\n",
      "0.815464872708\n",
      "0.333333332222\n",
      "0.543643249378\n",
      "0.7...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 91\t11\t68\t53\t25\t83\t34\t84\t95\t79\t24\t37\t39\t48\t96\t86\t12\t42\t47\t98\t19\t28\t8\t78\t100\t62\t97\t49\t69\t5\t71\t59\t82\t29...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'felixdae@gmail.com'# EMAIL \n",
    "STUDENT_TOKEN = 'IC50UU7wk9gGC79j'# TOKEN \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
